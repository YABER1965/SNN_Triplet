---
title: QEUR23_SMNRT24 – SOART3メトリックスを距離に使ったSNNモデルで予測する(外観検査)
date: 2025-01-09
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "NSOARTC", "データセット", "外観検査", "Vision language Model"]
excerpt: Siamese Networkをやってみる
---

## QEUR23_SMNRT24 – SOART3メトリックスを距離に使ったSNNモデルで予測する(外観検査)

## ～ やっぱり、Embeddingはいい！(再確認) ～

QEU:FOUNDER ： “それでは、Siamese Neural Networkを使って、予測精度をさらに上げるための実験をします。今回は、ちょっと面白いことをしますよ。”

**（前回のパフォーマンス評価）**

![imageSMR2-15-1](/2025-01-09-QEUR23_SMNRT24/imageSMR2-15-1.jpg)

D先生 ： “あれ？Tripletに移行するのかと思えば、意外でしたね。”

QEU:FOUNDER ： “我々のやりたいことは、「しくみそのもの」ではなく、**距離の改善**なんです。”

![imageSMR2-15-2](/2025-01-09-QEUR23_SMNRT24/imageSMR2-15-2.jpg)

D先生 ： “確か、今回のシステムで使っている距離は、画像の属性が一致している場合には**「distance=1」**、そして、不一致の場合には**「distance=0」**です。”

QEU:FOUNDER ： “この距離の概念が、今回のプログラムでどのように変わるのかについて、すこし見てみましょう。例によって、「（コードの）切り抜き」だけを晒します。”

```python
# ---
# Install and import required dependencies
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset, Subset
# ---
import math
import time
import multiprocessing

# ---
####################################
# HUGGINGFACE DATASET を生成する
####################################
# ---
# NumPy配列からHugging Face Datasetの作成
data_dict = {
    "features": list(map(list, shuffled_X)),  # 各レコードの特徴をリストに変換
    "label": shuffled_label.tolist()                # ラベルをリストに変換
}

# Hugging Face Datasetの作成
dataset = Dataset.from_dict(data_dict)
transformed_dataset = dataset.with_format("torch")

# Apply transformations
def collate_fn(example):
    example["image"] = example["features"].reshape(36,39)
    example["label"] = set_label.index(example["label"])
    return example

# ---
transformed_dataset = transformed_dataset.map(collate_fn, num_proc=multiprocessing.cpu_count())
transformed_dataset = transformed_dataset.remove_columns("features")

# ---
##################################
# NETWORKS FUNCTIONS
##################################
# ---
# build model
class EmbeddingNet(nn.Module):
    def __init__(self):
        super(EmbeddingNet,self).__init__()
        self.linear1 = nn.Linear(36*39, 512) # 36 * 39 is the pixels for each image
        self.linear2 = nn.Linear(512, 256) #100 and 5o
        self.linear3 = nn.Linear(256, 256) #100 and 5o
        self.final = nn.Linear(256, 128)
        self.relu = nn.ReLU()
# Here we are just feeding the data into each layer, and returning the output, while defining an in-stance of this class.
# ここでは、このクラスのインスタンスを定義しながら、各レイヤーにデータを入力し、出力を返すだけです。
    def forward(self, img): #convert + flatten
        x = img.view(-1, 36*39)
        x = self.relu(self.linear1(x))
        x = self.relu(self.linear2(x))
        x = self.relu(self.linear3(x))
        x = self.final(x)
        return x

    def get_embedding(self, x):
        return self.forward(x)

class SiameseNet(nn.Module):
    def __init__(self, embedding_net):
        super(SiameseNet, self).__init__()
        self.embedding_net = embedding_net

    def forward(self, x1, x2):
        output1 = self.embedding_net(x1)
        output2 = self.embedding_net(x2)
        return output1, output2

    def get_embedding(self, x):
        return self.embedding_net(x)

################################
# MODEL INSTANCE SETTING
################################
# ---
# Set up the network and training parameters
# ---
embedding_net = EmbeddingNet()
model_pred = SiameseNet(embedding_net)
cuda = torch.cuda.is_available()
if cuda:
    model_pred.cuda()

# ----
# モデル定義が必要
model_pred.load_state_dict(torch.load('drive/MyDrive/siamese_gaikan_model.pth'))
model_pred.eval()

#SiameseNet(
#  (embedding_net): EmbeddingNet(
#    (linear1): Linear(in_features=1404, out_features=512, bias=True)
#    (linear2): Linear(in_features=512, out_features=256, bias=True)
#    (linear3): Linear(in_features=256, out_features=256, bias=True)
#    (final): Linear(in_features=256, out_features=128, bias=True)
#    (relu): ReLU()
#  )
#)

# ---
# 距離は切り替え可能！
from scipy.spatial.distance import chebyshev, minkowski

# ---
# soaRT3メトリックスを計算する
def calc_soaRT3(tsr_sig_array, tsr_tani_array): 

    # データの抽出
    y = tsr_sig_array
    x = tsr_tani_array
    #print(y)
    #print(x)

    # dot回転を計測
    xx = np.dot(x,x) + 0.0001
    xy = np.dot(x,y) + 0.0001
    beta = xy/xx

    # ---
    # ミンコフスキー距離（Minkowski Distance）
    vDistance = minkowski(y,beta*x, 4) # p=4 

    # 絶対値（マンハッタン）距離を計測
    mDistance = minkowski(y,beta*x, 1) # p=1 
    #print("mDistance: ", mDistance.item())
    
    # 値の変換
    log_beta = beta
    log_yita = math.log(mDistance+1.0)
    log_gamma = log_yita - math.log(vDistance+1.0)
    
    return round(log_beta,5), round(log_yita,5), round(log_gamma,5)

# ---
# 事前評価
mnist_dataset = transformed_dataset
train_labels = mnist_dataset["label"]
train_data = mnist_dataset["image"]
# ---
len_label = len(train_labels)
print("len_label: ", len_label)

# ---
arr_gamma  = []
arr_target_matched = []
arr_target_unmatch = []
for i in range(2000):
    idx_image1 = np.random.randint(len_label)
    idx_image2 = np.random.randint(len_label)
    # ---
    label_image1 = train_labels[idx_image1]
    label_image2 = train_labels[idx_image2]
    # ----
    # SOART3距離を計算する
    tsr_sig_array = torch.unsqueeze(train_data[idx_image1], 1)
    tsr_tani_array = torch.unsqueeze(train_data[idx_image2], 1)
    # ----
    # EMBEDDINGを生成する
    embedding_sig_array = model_pred.get_embedding(tsr_sig_array.cuda()).data.cpu().numpy()
    embedding_tani_array = model_pred.get_embedding(tsr_tani_array.cuda()).data.cpu().numpy()
    #print(embedding_sig_array)
    # ---
    log_beta, log_yita, log_gamma = calc_soaRT3(embedding_sig_array[0], embedding_tani_array[0])
    # ---
    arr_gamma.append(log_gamma)
    # ---
    if label_image1 == label_image2:
        target = max([0.5*log_gamma-0.25,0.0])
        arr_target_matched.append(target)
    else:
        target = max([0.5*log_gamma-0.25,0.0])
        arr_target_unmatch.append(target)

# ---
# HISTグラフの描画(GAMMA)
# ---
fig = plt.figure(figsize=[8, 6])
plt.title(' GAMMA DISTRIBUTION ',fontsize = 20)
plt.hist(arr_target_matched, alpha=0.5, bins=20, label="Matched", color='b') # (3) ヒストグラムの描画(男性) 
plt.hist(arr_target_unmatch, alpha=0.3, bins=20, label="Unmatch", color='r')    # (4) ヒストグラムの描画(女性)
plt.legend(loc="upper left", fontsize=13) # (5)凡例表示
plt.show()

```

D先生 ： “FOUNDERは、前から予告していましたね。今回は、距離として**SOART3の高次メトリックス(GAMMA)を使いました**。でも、すこし変な処理をしていますよね。”

QEU:FOUNDER ： “そうすると、以下のような距離値の分布を生成することができるんです。”

![imageSMR2-15-3](/2025-01-09-QEUR23_SMNRT24/imageSMR2-15-3.jpg)

QEU:FOUNDER ： “ちなみに、モデルから出て来たEmbeddingをそのままMinkovski処理すると、「TUNE前」のような分布になります。それではダメなので、Unmatchとmatched群の山が重ならないように調整をしたんです。”

D先生 ： “「TUNE前」の距離を使うと、予測がボロボロになるでしょうね。画像の一致を識別できていないわけだし・・・。”

QEU:FOUNDER ： “そです。実際に「TUNE前」を使ったテストをやってみました。もう、ボロボロな結果です・・・（笑）。さて、小生がTUNEをした後のモデルで、どのようなアウトプット（モデルの学習）が出てくるのかを見てみましょう。まずは、生成したEMBEDDINGの可視化です。PCA散布図のプログラムは以前のモノと同じです。”

![imageSMR2-15-4](/2025-01-09-QEUR23_SMNRT24/imageSMR2-15-4.jpg)

D先生 ： “前回のEmbeddingの分布と全く違いますね。合格品「Label=0」と、その他の不合格品「label != 0」との差が明確になってきました。逆に、予測のパフォーマンスから見ると悪い方向に移行したのではないかなあ・・・？”

**（前回の結果: Siamese-128）**

![imageSMR2-15-5](/2025-01-09-QEUR23_SMNRT24/imageSMR2-15-5.jpg)

QEU:FOUNDER ： “じゃあ、Confusion Matrixを見てみましょう。もちろん、SOART3距離を使用して学習したモデルを読み込んで、そのEmbedding情報から予測をしています。”

![imageSMR2-15-6](/2025-01-09-QEUR23_SMNRT24/imageSMR2-15-6.jpg)

D先生 ： “あれ？**予測のパフォーマンスがさらに良くなった！！**”

![imageSMR2-15-7](/2025-01-09-QEUR23_SMNRT24/imageSMR2-15-7.jpg)

QEU:FOUNDER ： “なぜパフォーマンスが良くなったのかについて解説しましょう。Confusion Matrixによれば、予測の誤りのほとんどは、**「不合格品を合格品であると予測した」**ことにあります。そして、EmbeddingのPCA散布図を見ると、今回の分布の方が合格品のクラスタと不合格品のクラスタが離れていったんですよ。”

D先生 ： “そういう仮説も成り立ちますね。もしそうならば、もう一回、SOARTを使って同様に処理すれば、さらに予測精度が良くなることになります。”

QEU:FOUNDER ： “まあ、確認として、もう一度やってみましょう。SOART3の高次元メトリックスについて、再考が必要であると思うし・・・。”


## ～ まとめ ～

QEU:FOUNDER ： “例のビックイベントの件、開幕まで、あと100日を切ったのか・・・。もう、ワクワクしますね。”

[![MOVIE1](http://img.youtube.com/vi/fGRSXsYbY7k/0.jpg)](http://www.youtube.com/watch?v=fGRSXsYbY7k "壊滅的！大阪万博、あと100日、チケットまったく売れず。")

QEU:FOUNDER ： “どんなイベントなのかなあ・・・。早く知りたいなあ・・・。”

C部長 : “そんな重要なコトを知らないんですか？すでに、Web情報が一部漏れてきていますよ！ “

[![MOVIE2](http://img.youtube.com/vi/BGzGjEecFqg/0.jpg)](http://www.youtube.com/watch?v=BGzGjEecFqg "「万博あと100日 空から展望」リングの外"100 Days Left for the Expo: A View from the Sky" Out-side the Ring")

QEU:FOUNDER ： “おおっ、こんなものができているのか・・・。それにしても、右側の池（↓）って排水用？臭くないの？”

![imageSMR2-15-8](/2025-01-09-QEUR23_SMNRT24/imageSMR2-15-8.jpg)

C部長 : “そんな、つまらない部分は見ないでください！**Think POSITIVE!!!** ホレ！すごいロボットが立っているでしょ！！ “

![imageSMR2-15-9](/2025-01-09-QEUR23_SMNRT24/imageSMR2-15-9.jpg)

QEU:FOUNDER ： “これ（↑）のことか？ずいぶん小さいなあ・・・。左側の変な建物（↓）はなんだ！？”

![imageSMR2-15-10](/2025-01-09-QEUR23_SMNRT24/imageSMR2-15-10.jpg)

C部長 : “は？FOUNDERは、なんにも知らないんですね。これは有名なトイレですよ！**「2億円トイレ」として一時期有名になった**。実際には、とれほどお金を使っていないそうです。 “

![imageSMR2-15-11](/2025-01-09-QEUR23_SMNRT24/imageSMR2-15-11.jpg)

C部長 : “FOUNDER・・・。もっと前向きに物事を見てくださいよ。ホラホラ・・・。FOUNDERの大好きなガンダム（↓）がありますよ！ “

![imageSMR2-15-12](/2025-01-09-QEUR23_SMNRT24/imageSMR2-15-12.jpg)

QEU:FOUNDER ： “なぜ**膝をついている**んですか？横浜のモノは動いていたし、立っていましたよ。わざわざ、それを壊しちゃって・・・。もう、もったいない・・・。”

C部長 : “膝をついているのは、**「もし、（G像が）立っていると地面に沈むから」**という噂があります。ここの地盤が弱いんですよ。FOUNDERは、スリル（↓）を味わうことがすきでしょ？“

![imageSMR2-15-13](/2025-01-09-QEUR23_SMNRT24/imageSMR2-15-13.jpg)

QEU:FOUNDER ： “ギャー！！**巨石が落ちてくる日よけ（↑）**だ！！”

C部長 : “そう思えば、たとえ猛暑の参観でも**涼しくなる**でしょ？その心理的効果を狙ったという噂もあります。どう？面白いでしょう？行きたくなるでしょう？世界中の**「おいしいたべ物（↓）」**を食べられるんですよ。“

![imageSMR2-15-14](/2025-01-09-QEUR23_SMNRT24/imageSMR2-15-14.jpg)

QEU:FOUNDER ： “う～ん・・・。もしも、**お客が来なかったらどうするんだろう・・・。**”

C部長 : “（イベントが）始まる前から、そんなネガティブなことを想像してもいけないし、ましてや発言してもいけないんです！！ “

QEU:FOUNDER ： “いやいや・・・。ふと思ったものですから・・・。”

![imageSMR2-15-15](/2025-01-09-QEUR23_SMNRT24/imageSMR2-15-15.jpg)

C部長 : “もし、万が一に大損失があれば、責任者が**「ソ連のマネをしたかった」**といえばいいんでしょう。周りが関西流にずっこけてくれますよ。“

