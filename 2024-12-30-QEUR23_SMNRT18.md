---
title: QEUR23_SMNRT18 : PyTorchとSIMPLEなモデルでEmbeddingを生成する(MNIST)
date: 2024-12-30
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "NSOARTC", "データセット", "外観検査", "Vision language Model"]
excerpt: Siamese Networkをやってみる
---

## QEUR23_SMNRT18 : PyTorchとSIMPLEなモデルでEmbeddingを生成する(MNIST)

## ～ 普通のモデルでもEmbeddingを生成できる！ ～

D先生 ： “前回までは、**Kerasを使って外観検査画像の異常検出の評価をしました**。その結果、Siamese Neural Network(SNN)では、その異常検出能力には限界があることがわかりました。”

![imageSMR2-9-1](/2024-12-30-QEUR23_SMNRT18/imageSMR2-9-1.jpg)

QEU:FOUNDER ： “現状のSNNは、異常の見逃しが発生しやすい構造であり、**ユーグリッド距離を使った場合は画像の一部だけに発生した小さな異常を発見しにくい**んです。そして、残念ながら・・・。少なくとも我々のプログラミングの能力では、Kerasを使った、これ以上のシステムの大改造は難しいようです。そこで、**カスタム性が高いPytorchに乗り換える**必要がありますね。”

![imageSMR2-9-2](/2024-12-30-QEUR23_SMNRT18/imageSMR2-9-2.jpg)

D先生 ： “すなわち、**SNNの距離をSOART3の高次距離に変える**ということですね。”

QEU:FOUNDER ： “Web上には探せばあるもので、我々にとって、**なかなかに有望なPytorchのテンプレート・プログラム**を見つけました。これは、古いのですが、とてもいいですよ！これは、なんと！！今までにない発見もありました！！！”

![imageSMR2-9-3](/2024-12-30-QEUR23_SMNRT18/imageSMR2-9-3.jpg)

D先生 ： “ほう・・・。どんな部分が有望ですか？”

QEU:FOUNDER ： “まあ、このプログラムを実行すればわかります。ただし、小生は目的に適合させるために、プログラムをかなり変えています。まずは、コードの序盤をドン！！”

```python
# ---
# Common setup
import torch
from torch.optim import lr_scheduler
import torch.optim as optim
from torch.autograd import Variable
# ---
import torch.nn as nn
import torch.nn.functional as F
# ---
import numpy as np
import random
import matplotlib
import matplotlib.pyplot as plt
# ---
# Prepare dataset
from torchvision.datasets import MNIST
from torchvision import transforms

# ---
mean, std = 0.1307, 0.3081
train_dataset = MNIST('../data/MNIST', train=True, download=True,
                             transform=transforms.Compose([
                                 transforms.ToTensor(),
                                 transforms.Normalize((mean,), (std,))
                             ]))
test_dataset = MNIST('../data/MNIST', train=False, download=True,
                            transform=transforms.Compose([
                                transforms.ToTensor(),
                                transforms.Normalize((mean,), (std,))
                            ]))

# ---
# Function to display random MNIST images
def display_random_mnist_images(dataset, num_images=4):
    # Randomly select indices
    indices = random.sample(range(len(dataset)), num_images)
    
    # Create a figure for displaying images
    plt.figure(figsize=(12, 7))
    
    for i, idx in enumerate(indices):
        # Get the image and label
        image, label = dataset[idx]
        print(image.shape)
        
        # Convert the image tensor to numpy array and denormalize
        image = image.numpy().squeeze()  # Remove channel dimension
        
        # Display the image
        plt.subplot(1, num_images, i + 1)
        plt.imshow(image, cmap='gray')
        plt.title(f'Label: {label}')
        plt.axis('off')
    
    plt.tight_layout()
    plt.show()

# ---
# Display 4 random MNIST images
display_random_mnist_images(test_dataset, num_images=4)

```

D先生 ： “例の手書き文字MNIST(28x28)ですね。これは、普通の画像判別の問題です。”

![imageSMR2-9-4](/2024-12-30-QEUR23_SMNRT18/imageSMR2-9-4.jpg)

QEU:FOUNDER ： “そうです。これを改造して、次回では外観検査に改造します。つぎは、いよいよモデルの学習に入りましょう。”

```python
# ---
n_classes = 10
mnist_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',
              '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',
              '#bcbd22', '#17becf']

##################################
# NETWORKS FUNCTIONS
##################################
# ---
# build model
class EmbeddingNet(nn.Module):
    def __init__(self):
        super(EmbeddingNet,self).__init__()
        self.linear1 = nn.Linear(28*28, 256) #28 * 28 is the pixels for each image
        self.linear2 = nn.Linear(256, 256) #100 and 5o
        self.final = nn.Linear(256, 2)
        self.relu = nn.ReLU()
# このクラスのインスタンスを定義しながら、各レイヤーにデータを入力し、出力を返すだけです。
    def forward(self, img): #convert + flatten
        x = img.view(-1, 28*28)
        x = self.relu(self.linear1(x))
        x = self.relu(self.linear2(x))
        x = self.final(x)
        return x

    def get_embedding(self, x):
        return self.forward(x)

# ---
class ClassificationNet(nn.Module):
    def __init__(self, embedding_net, n_classes):
        super(ClassificationNet, self).__init__()
        self.embedding_net = embedding_net
        self.n_classes = n_classes
        self.nonlinear = nn.PReLU()
        self.fc1 = nn.Linear(2, n_classes)

    def forward(self, x):
        output = self.embedding_net(x)
        output = self.nonlinear(output)
        scores = F.log_softmax(self.fc1(output), dim=-1)
        return scores

    def get_embedding(self, x):
        return self.nonlinear(self.embedding_net(x))

##################################
# METRICS FUNCTIONS
##################################
# ---
class Metric:
    def __init__(self):
        pass

    def __call__(self, outputs, target, loss):
        raise NotImplementedError

    def reset(self):
        raise NotImplementedError

    def value(self):
        raise NotImplementedError

    def name(self):
        raise NotImplementedError

class AccumulatedAccuracyMetric(Metric):
    """
    Works with classification model
    """
    def __init__(self):
        self.correct = 0
        self.total = 0

    def __call__(self, outputs, target, loss):
        pred = outputs[0].data.max(1, keepdim=True)[1]
        self.correct += pred.eq(target[0].data.view_as(pred)).cpu().sum()
        self.total += target[0].size(0)
        return self.value()

    def reset(self):
        self.correct = 0
        self.total = 0

    def value(self):
        return 100 * float(self.correct) / self.total

    def name(self):
        return 'Accuracy'

##################################
# TRAIN - FIT FUNCTIONS
################################
# ---
def train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics):
    for metric in metrics:
        metric.reset()

    model.train()
    losses = []
    total_loss = 0

    for batch_idx, (data, target) in enumerate(train_loader):
        target = target if len(target) > 0 else None
        if not type(data) in (tuple, list):
            data = (data,)
        if cuda:
            data = tuple(d.cuda() for d in data)
            if target is not None:
                target = target.cuda()

        optimizer.zero_grad()
        outputs = model(*data)

        if type(outputs) not in (tuple, list):
            outputs = (outputs,)

        loss_inputs = outputs
        if target is not None:
            target = (target,)
            loss_inputs += target

        loss_outputs = loss_fn(*loss_inputs)
        loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs
        losses.append(loss.item())
        total_loss += loss.item()
        loss.backward()
        optimizer.step()

        for metric in metrics:
            metric(outputs, target, loss_outputs)

        if batch_idx % log_interval == 0:
            message = 'Train: [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                batch_idx * len(data[0]), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), np.mean(losses))
            for metric in metrics:
                message += '\t{}: {}'.format(metric.name(), metric.value())

            print(message)
            losses = []

    total_loss /= (batch_idx + 1)
    return total_loss, metrics

# ---
def test_epoch(val_loader, model, loss_fn, cuda, metrics):
    with torch.no_grad():
        for metric in metrics:
            metric.reset()
        model.eval()
        val_loss = 0
        for batch_idx, (data, target) in enumerate(val_loader):
            target = target if len(target) > 0 else None
            if not type(data) in (tuple, list):
                data = (data,)
            if cuda:
                data = tuple(d.cuda() for d in data)
                if target is not None:
                    target = target.cuda()

            outputs = model(*data)

            if type(outputs) not in (tuple, list):
                outputs = (outputs,)
            loss_inputs = outputs
            if target is not None:
                target = (target,)
                loss_inputs += target

            loss_outputs = loss_fn(*loss_inputs)
            loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs
            val_loss += loss.item()

            for metric in metrics:
                metric(outputs, target, loss_outputs)

    return val_loss, metrics

# ---
def fit(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[],
        start_epoch=0):
    """
    ローダー、モデル、損失関数、およびメトリックは、種々のタスク(Simple, Siamese, Triplet)に応じて、適当なモジュールを組み合わせて動作する必要があります。
    モデルはローダーのデータ出力を処理でき、損失関数はローダーのターゲット出力とモデルからの出力を処理できる必要があります。
    """
    for epoch in range(0, start_epoch):
        scheduler.step()

    for epoch in range(start_epoch, n_epochs):
        scheduler.step()

        # Train stage
        train_loss, metrics = train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics)

        message = 'Epoch: {}/{}. Train set: Average loss: {:.4f}'.format(epoch + 1, n_epochs, train_loss)
        for metric in metrics:
            message += '\t{}: {}'.format(metric.name(), metric.value())

        val_loss, metrics = test_epoch(val_loader, model, loss_fn, cuda, metrics)
        val_loss /= len(val_loader)

        message += '\nEpoch: {}/{}. Validation set: Average loss: {:.4f}'.format(epoch + 1, n_epochs,
                                                                                 val_loss)
        for metric in metrics:
            message += '\t{}: {}'.format(metric.name(), metric.value())

        print(message)

#######################################
# MAIN - NORMAL
#######################################
# ---
# PARAMETER SETTING
embedding_net = EmbeddingNet()
model = ClassificationNet(embedding_net, n_classes=n_classes)
cuda = torch.cuda.is_available()
if cuda:
    model.cuda()
loss_fn = torch.nn.NLLLoss()
lr = 2e-3
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)
n_epochs = 10
log_interval = 50

########################################
# CREATE DATALOADER
########################################
# ---
# Baseline: Classification with softmax
# Set up data loaders
batch_size = 256
kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, **kwargs)

#######################################
# TRAINING
#######################################
# ---
fit(train_loader, test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, met-rics=[AccumulatedAccuracyMetric()])

```

D先生 ： “う～ん。これと、元のテンプレートのコードと比較すると・・・。どうやら、FOUNDERは、わざと**ディ―プ・ラーニング(DL)のモデルをシンプルにした**ようです。それでも、かなり高い正確度を出してくれています。”

![imageSMR2-9-5](/2024-12-30-QEUR23_SMNRT18/imageSMR2-9-5.jpg)

QEU:FOUNDER ： “元祖のテンプレートは、DLモデルに畳み込みを使用しています。ただし、我々は**外観検査ではNSOARTCで生成した1次元ベクトルを使っている**ので、このままでは使いづらいんです。ちなみに、このテンプレートのように畳み込みを使えば、MNISTの判別では、すぐに100％の正確度をたたき出していきます。さて、ここで、いよいよ今回の大発見です。注意、小生にとってはね・・・。それでは、プログラムをドン！！”

```python
##########################################
# RVALUATING BY EMBEDDING
##########################################
# ---
def plot_embeddings(embeddings, targets, xlim=None, ylim=None):
    plt.figure(figsize=(10,10))
    for i in range(10):
        inds = np.where(targets==i)[0]
        plt.scatter(embeddings[inds,0], embeddings[inds,1], alpha=0.5, color=colors[i])
    if xlim:
        plt.xlim(xlim[0], xlim[1])
    if ylim:
        plt.ylim(ylim[0], ylim[1])
    plt.legend(mnist_classes)

def extract_embeddings(dataloader, model):
    with torch.no_grad():
        model.eval()
        embeddings = np.zeros((len(dataloader.dataset), 2))
        labels = np.zeros(len(dataloader.dataset))
        k = 0
        for images, target in dataloader:
            if cuda:
                images = images.cuda()
            embeddings[k:k+len(images)] = model.get_embedding(images).data.cpu().numpy()
            labels[k:k+len(images)] = target.numpy()
            k += len(images)
    return embeddings, labels

# ---
train_embeddings_baseline, train_labels_baseline = extract_embeddings(train_loader, model)
plot_embeddings(train_embeddings_baseline, train_labels_baseline)
val_embeddings_baseline, val_labels_baseline = extract_embeddings(test_loader, model)
plot_embeddings(val_embeddings_baseline, val_labels_baseline)

```

QEU:FOUNDER ： “じつは、SNNを使わずともEmbeddingを生成することができるんです。”

![imageSMR2-9-6](/2024-12-30-QEUR23_SMNRT18/imageSMR2-9-6.jpg)

D先生 ： “これは便利だ・・・。ちなみに、この散布図では、Embeddingベクトルの第一要素と第二要素をプロットしたのですね。このEmbeddingベクトルは、他のソルーションにも使えるんじゃないですか？ホレ・・・。FOUNDERのような**ディープな人**が大好きな・・・。”

![imageSMR2-9-7](/2024-12-30-QEUR23_SMNRT18/imageSMR2-9-7.jpg)

QEU:FOUNDER ： “あれのことかい？確かに、多変量正規分布のみに依拠しているマハラノビス距離は、多変量や非線形にうまく対応できません。**このEmbeddingを前処理につかうと使える範囲は大きく広がる**と思いますよ。”

![imageSMR2-9-8](/2024-12-30-QEUR23_SMNRT18/imageSMR2-9-8.jpg)

QEU:FOUNDER ： “例えば、単位空間が多い場合には、第一段階で単位空間に分類されるようにディープラーニングに学習させます。その後の第二段階で、出力されたEmbeddingベクトルで異常検出をします。今回は、PyTorchによるSNN学習の準備体操ですが、バッチリのスタートでしょう？”

![imageSMR2-9-9](/2024-12-30-QEUR23_SMNRT18/imageSMR2-9-9.jpg)

D先生 ： “複雑系に一歩近づいた！！今回のシリーズでは、画像判別問題(MNIST)と異常検出(TANSHI)を比較しながら進めていけば、より分かりやすくなりますね。”


## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “**J国の「経済のソ連化」というよりも、「社会のソ連化」ですね**。”

![imageSMR2-9-10](/2024-12-30-QEUR23_SMNRT18/imageSMR2-9-10.jpg)

C部長 : “あ～あ・・・。言っちゃった・・・。“

![imageSMR2-9-11](/2024-12-30-QEUR23_SMNRT18/imageSMR2-9-11.jpg)

QEU:FOUNDER ： “年末になって、大きな動きがありましたね。”

C部長 : “おそらく海外の方が、よくわかっているんですよ。あそこで、一体。何が起こっているのか・・・。しかし、FOUNDER・・・。せっかくなので、もっと景気の良い話はないんですか？来年に向けて・・・。”

[![MOVIE1](http://img.youtube.com/vi/n3S8eVHkzvY/0.jpg)](http://www.youtube.com/watch?v=n3S8eVHkzvY "【2025年超予測：生成AI】松尾研OBたち／AIエージェント元年／自分専用AIが広がる/学ぶデータの限界／イーロン・マスクの策略／正義のAI vs 悪のAI／AIが人間を洗脳")

QEU:FOUNDER ： “この動画（↑）なんかは、かなり面白いんじゃないですか？AI業界の若手のホープが**「ごく近い未来のAIの方向性」**について議論しています。いわゆる「AI」ってものが、人間の思考やコミュニケーションをサポートする「エージェント」に発展するとしています。”

C部長 : “ん・・・！？これは、既視感があるなぁ・・・（笑）。 “

![imageSMR2-9-12](/2024-12-30-QEUR23_SMNRT18/imageSMR2-9-12.jpg)

QEU:FOUNDER ： “**いま、我々が推進している「BONSAI4」がそれです**。BONSAIは、ディベートを通じて情報を収集してLLMモデルを学習するんです。ここで重要なのは、BONSAIは、ある議題において2つの異なる意見を設定することです。そうすると、**ある議題に関して「POSITIVE(自分の意見に沿う)」と「NEGATIVE(自分の意見に沿わない)」の情報が収集できる**んです。”

![imageSMR2-9-13](/2024-12-30-QEUR23_SMNRT18/imageSMR2-9-13.jpg)

C部長 : “BONSAI4では、従来の学習された(UNSLOTH) LLMだけでなく、**RAGで使用するEmbeddingモデルも学習する**んですよね。“

QEU:FOUNDER ： “ここがBONSAI4の先進的なところです。小生は、ウソは言っていません。もしも、先ほどの動画が言っていることが本当に「先進的である」ならばね・・・（笑）。通常の方法は、適当にfinetuningのための情報を入力してモデルを学習していけば、RAGを通じて、学習情報に関連した情報が手に入って、LLMが出力するでしょう。ただし、それはBONSAIにおいては**「議題(AGENDA)」**にすぎません。つまり、その学習済みのEmbeddingモデルには、**「自分の意図に合っているかどうか？(POSITIVE-NEGATIVE)」という評価情報が入っていません**。もし、「POSITIVE-NEGATIVE」の情報がモデルに入っていれば、自分が意図しないNEGATIVEな情報の収集を排除できるんです。”

![imageSMR2-9-14](/2024-12-30-QEUR23_SMNRT18/imageSMR2-9-14.jpg)

C部長 : “だからこそ、このBONSAI4のEmbeddingモデルの学習では、Tripletを使用する予定ですよね？“

QEU:FOUNDER ： “一応は、その予定です。・・・でも、tripletの採用だけで、BONSAI4の考えるゴールが達成できるかどうかはわかりません。さて、今年も押し迫ってきたことだし、次は、来年度において、外観検査のプロジェクトがどのような発展を遂げるのかについても説明しましょう。久々に、「まとめ」らしく技術的な会話だ・・・（笑）。”

