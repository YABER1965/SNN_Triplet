---
title: QEUR23_BS4RVW6 - Phi-4を使って強化学習Finetuningを試す(Unsloth-GRPO)
date: 2025-02-20
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "LLM", "データセット", "BONSAI", "DeepSeek"]
excerpt: あたらしいLLMの学習体系を確立する
---

## QEUR23_BS4RVW6 - Phi-4を使って強化学習Finetuningを試す(Unsloth-GRPO)

## ～ GRPOは、すごい！悩ましい。「出発点」にすぎない・・・ ～

QEU:FOUNDER （設定年齢65歳）： “いきなり、Unsloth様が**「すごい学習ツール」をリリース**したよ！！”

![imageBSR1-7-1](/2025-02-20-QEUR23_BS4RVW6/imageBSR1-7-1.jpg) 

D先生（設定年齢65歳）： “なるほど、これが**秘密兵器**か！！この強化学習で、例のDeepSeek R1は完成したんですか。これは、我々も**「ちょっと試しにやってみる」**しかないです。“

![imageBSR1-7-2](/2025-02-20-QEUR23_BS4RVW6/imageBSR1-7-2.jpg) 

QEU:FOUNDER  ： “今回、彼ら(Unsloth team)も特に力を入れているのか、**対応モデル別にColab Notebookを作っています**。これを参考にしてやりましょう。ただし、このままではうまくモデルは動かないよ。たぶんね・・・。”

D先生： “えっ！？どういうこと？ “

QEU:FOUNDER ： “あとで説明します。まずは、Colabプログラムの抜粋をドン！詳細はネタ元をみてください。コレ（↓）は序盤です。我々は、多国語対応をしたいので、モデルが大きめの**Phi4-14B**を使っています。”

```python
# ---
from unsloth import FastLanguageModel, PatchFastRL
PatchFastRL("GRPO", FastLanguageModel)

# ---
# Load up Phi-4 14B, and set parameters
# Phi-4 14Bをロードし、パラメータを設定します
from unsloth import is_bfloat16_supported
import torch
max_seq_length = 512 * 7 # Can increase for longer reasoning traces
lora_rank = 16 # Larger rank = smarter, but slower

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Phi-4",
    max_seq_length = max_seq_length,
    load_in_4bit = True, # False for LoRA 16bit
    fast_inference = True, # Enable vLLM fast inference
    max_lora_rank = lora_rank,
    gpu_memory_utilization = 0.8, # Reduce if out of memory
)

model = FastLanguageModel.get_peft_model(
    model,
    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["gate_proj", "up_proj", "down_proj",],
    lora_alpha = lora_rank,
    use_gradient_checkpointing = "unsloth", # Enable long context finetuning
    random_state = 3442,
)

# ---
# Data Prep
# データの準備とすべての報酬関数には @willccbb を直接活用しています。独自の関数を自由に作成できます。
import re
from datasets import load_dataset, Dataset

# Load and prep dataset
SYSTEM_PROMPT = """
Respond in the following format:
<reasoning>
...
</reasoning>
<answer>
...
(</answer>
"""

XML_COT_FORMAT = """\
<reasoning>
{reasoning}
(/reasoning)
<answer>
{answer}
</answer>
"""

def extract_xml_answer(text: str) -> str:
    answer = text.split("(answer)")[-1]
    answer = answer.split("(/answer)")[0]
    return answer.strip()

def extract_hash_answer(text: str) -> str | None:
    if "####" not in text:
        return None
    return text.split("####")[1].strip()

# uncomment middle messages for 1-shot prompting
def get_gsm8k_questions(split = "train") -> Dataset:
    #data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore
    data = load_dataset('YxBxRyXJx/QAPHI4_GRPO_250218', 'default')[split] # type: ignore
    data = data.map(lambda x: { # type: ignore
        'prompt': [
            {'role': 'system', 'content': SYSTEM_PROMPT},
            {'role': 'user', 'content': x['question']}
        ],
        'answer': extract_hash_answer(x['answer'])
    }) # type: ignore
    return data # type: ignore

dataset = get_gsm8k_questions()

```

D先生： “例によって、**QEU(自分)用のデータセットを作った**んですね。ORPO用のデータセットは複雑なので、今回は間に合わないと思っていたのに・・・。”

![imageBSR1-7-3](/2025-02-20-QEUR23_BS4RVW6/imageBSR1-7-3.jpg) 

D先生： “おやおや・・・？なんと！！GRPOって、**強化学習なのにSFT（教師あり学習）と同じデータセットでやれる**んですね！これは便利だ！！ “

QEU:FOUNDER ： “その代わり、GPUハードウェアへの要求は高いですよ。小生は、それに伴い、やむなく**A100のGPU**を使いました。”

D先生： “あれ？UnslothのNotebookでは、T4レベルのGPUでやれると言っていませんでしたか？ “

QEU:FOUNDER ： “今回、我々は、DB読み込みとRespnse出力の文の量をNotebookの設定よりも大きくしているんです。実は、このNotebookの設定は、とてもじゃないが実用性がないです。それでは、プログラムの晒しのつづきにいきましょう。ただし、強化学習用の関数群は省略します。”

```python
# ---
# Train the model
# GRPO トレーナーとすべての構成をセットアップします。
# ---
from trl import GRPOConfig, GRPOTrainer
training_args = GRPOConfig(
    use_vllm = True, # use vLLM for fast inference!
    learning_rate = 7e-6,
    adam_beta1 = 0.9,
    adam_beta2 = 0.99,
    weight_decay = 0.1,
    warmup_ratio = 0.1,
    lr_scheduler_type = "cosine",
    optim = "paged_adamw_8bit",
    logging_steps = 1,
    bf16 = is_bfloat16_supported(),
    fp16 = not is_bfloat16_supported(),
    per_device_train_batch_size = 1,
    gradient_accumulation_steps = 1, # Increase to 4 for smoother training
    num_generations = 4, # Decrease if out of memory
    max_prompt_length = 256*6,
    max_completion_length = 200*6,
    # num_train_epochs = 1, # Set to 1 for a full training run
    max_steps = 100,
    save_steps = 250,
    max_grad_norm = 0.1,
    report_to = "none", # Can use Weights & Biases
    output_dir = "outputs",
)

# ---
# トレーナーを実行してみましょう。上にスクロールすると、報酬の表が表示されます。目標は、報酬の列が増えるのを確認することです。
# モデルが正しいアクションを起こすには、本来は150 ～ 200 STEPほど待たなければならない場合があります。最初の 100STEPでは、おそらく報酬は 0 になります。しばらくお待ちください。
# ---
trainer = GRPOTrainer(
    model = model,
    processing_class = tokenizer,
    reward_funcs = [
        xmlcount_reward_func,
        soft_format_reward_func,
        strict_format_reward_func,
        int_reward_func,
        correctness_reward_func,
    ],
    args = training_args,
    train_dataset = dataset,
)
trainer.train()

```

QEU:FOUNDER ： “これ（↓）がGRPO学習の進捗状況です。コメントにもあったが、本来は**200STEPが必要**です。たった100STEP程度でどの程度のパフォーマンスになるのか・・・。あと、ここに**「completion_length」**という項目があるでしょう？この数字がNotebook設定では、最大200トークンになっていたんです。”

![imageBSR1-7-4](/2025-02-20-QEUR23_BS4RVW6/imageBSR1-7-4.jpg) 

D先生： “分類問題じゃない限り、**出力トークン量が200では実用的ではない**ですね。それにしても、100STEPも学習したんですね。まだ不足とはいえ、かなり長い時間（コスト）をかけましたね。 “

![imageBSR1-7-5](/2025-02-20-QEUR23_BS4RVW6/imageBSR1-7-5.jpg) 

QEU:FOUNDER ： “今回のトレーナの設計が面白く、**強化学習の過程でReasoning文を出力**しています。UnslothのNotebookを見てください。モデルがQuestion-Answerの情報を解析し、Reasoningを作成するようになっているんですね。それでは、次にモデルの推論をやっていましょう。ここでは、モデルを一旦保管しました、そのLoraのデータを読みだして推論をしてみましょう。”


```python
# ---
from unsloth import FastLanguageModel, PatchFastRL
PatchFastRL("GRPO", FastLanguageModel)

# ---
# Load up Phi-4 14B, and set parameters
# Phi-4 14Bをロードし、パラメータを設定します
from unsloth import is_bfloat16_supported
import torch
max_seq_length = 512 * 7 # Can increase for longer reasoning traces
lora_rank = 16 # Larger rank = smarter, but slower

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "drive/MyDrive/grpo_saved_lora",
    max_seq_length = max_seq_length,
    load_in_4bit = True, # False for LoRA 16bit
    fast_inference = True, # Enable vLLM fast inference
    max_lora_rank = lora_rank,
    gpu_memory_utilization = 0.8, # Reduce if out of memory
)

# ---
# Load and prep dataset
SYSTEM_PROMPT = """
Respond in the following format:
<reasoning>
...
</reasoning>
<answer>
...
</answer>
"""

#################################
# INFERENCE(1)
#################################
# ---
# Now we load the LoRA and test:
text = tokenizer.apply_chat_template([
    {"role" : "system", "content" : SYSTEM_PROMPT},
    {"role" : "user", "content" : "Which is bigger? 9.11 or 9.9?"},
], tokenize = False, add_generation_prompt = True)

from vllm import SamplingParams
sampling_params = SamplingParams(
    temperature = 0.5,
    top_p = 0.95,
    max_tokens = 1024*2,
)
output = model.fast_generate(
    text,
    sampling_params = sampling_params,
    lora_request = None,
)[0].outputs[0].text

output

```

D先生： “ここでは、ちゃんと**REASONINGがでてくる**んですか”

**(reasoning)**
To determine which number is bigger between 9.11 and 9.9, we need to compare them digit by digit starting from the leftmost digit.

1. Compare the whole number part:
   - Both numbers have the whole number part as 9.

2. Compare the tenths place:
   - 9.11 has a 1 in the tenths place.
   - 9.9 has a 9 in the tenths place.

Since 9 is greater than 1, 9.9 is greater than 9.11 based on the tenths place alone. There is no need to compare further digits.

**(/reasoning)**
**(answer)**
9.9 is bigger than 9.11.
**(/answer)**

D先生： “おっと！(Reasoningが)出た出た！！感動！！！”

QEU:FOUNDER ： “（このモデルは、数理ロジックには強そうですね。次に、コモンで、かつベーシックな例題として、**「旅行計画の設定」**をやってみましょう。”

```python
#################################
# INFERENCE(2)
#################################
# ---
# Now we load the LoRA and test:
text = tokenizer.apply_chat_template([
    {"role" : "system", "content" : SYSTEM_PROMPT},
    {"role" : "user", "content" : "日本発、香港の3泊4日の旅行スケジュールを設定してください。"},
], tokenize = False, add_generation_prompt = True)

# 中略
output

```

**(reasoning)**

香港は、その多様な文化、美しい景観、そして豊富な食事で知られる魅力的な都市です。3泊4日の旅行スケジュールを設定する際には、香港の主要な観光地を訪れつつ、現地の文化や生活を体験することを目指します。

1. **1日目: 香港島の探索**
   - **朝:** 香港国際空港から市内へ移動。ホテルにチェックイン後、香港島の中心部にある銅鑼湾（コーズウェイベイ）へ。
   - **午前中:** 銅鑼湾のショッピング街を散策。ディズニーランドやハリウッドマーライオンなどの有名な観光スポットを見学。
   - **昼食:** 銅鑼湾で地元の食堂や屋台で香港料理を楽しむ。
   - **午後:** ヴィクトリアピークへ。香港の壮大な景観を楽しみ、夕日を眺める。
   - **夕食:** 香港の伝統的な酒樓（シャンロウ）で食事を楽しむ。

2. **2日目: 香港島と九龍の探索**
   - **朝:** 香港島の香港大学付近にある香港博物館を訪れ、香港の歴史や文化に触れる。
   - **昼食:** 博物館近くのカフェで軽食。
   - **午後:** 九龍半島へ移動。尖沙咀（チムサーチョイ）でショッピングを楽しむ。
   - **夕方:** 香港ハーバークルーズに参加し、夜景を楽しむ。
   - **夕食:** 尖沙咀のレストランで夕食。

3. **3日目: 新界の探索**
   - **朝:** 大埔まで電車で移動し、香港仔（ホンコンヤイ）で海鮮料理を堪能。
   - **午前中:** ランタオ島へフェリーで移動し、ランタオ島の自然を楽しむ。
   - **昼食:** ランタオ島で地元のレストランで食事。
   - **午後:** 香港仔の獅子山（シーズン）公園でハイキングを楽しむ。
   - **夕食:** 香港仔のレストランで夕食。

4. **4日目: 帰路へ**
   - **朝:** ホテルでのんびりと過ごし、荷物をまとめる。
   - **昼食:** ホテル近くのレストランで最後の香港料理を楽しむ。
   - **午後:** 香港国際空港へ移動し、帰路へ。
   
**(/reasoning)**

**(answer)**

3泊4日の香港旅行スケジュールは以下の通りです。

1. **1日目: 香港島の探索**
   - 銅鑼湾でショッピングと観光。
   - ヴィクトリアピークで景色を楽しむ。
   - 伝統的な酒樓で夕食。

2. **2日目: 香港島と九龍の探索**
   - 香港博物館で文化を学ぶ。
   - 尖沙咀でショッピング。
   - 香港ハーバークルーズで夜景を楽しむ。

3. **3日目: 新界の探索**
   - 香港仔で海鮮料理を堪能。
   - ランタオ島で自然を楽しむ。
   - 獅子山公園でハイキング。

4. **4日目: 帰路へ**
   - ホテルでのんびり過ごし、最後の香港料理を楽しむ。
   - 香港国際空港へ移動し、帰路へ。
   
**(/answer)**

D先生： “ん！？なんじゃこれ？なんか、今回のREASONINGの使い方がうまくないですね。これはRESPONSEの内容と、ほとんど変わらないじゃないですか・・・。”

QEU:FOUNDER ： “あと、「3日目: 新界の探索」は、たった1日で実現可能なのか（笑）？なんか、これは「REASONING段階で一度試しに推論を出力し、それを取りまとめた」ような感じですね。ちなみに、**「旅行計画の例題」**は合計3回試しましたが、すべて同じ傾向です。もちろん、推論の結果自体はうまく行っています。それでは、推論テストがつづきます。”

```python
#################################
# INFERENCE(5)
#################################
# ---
# Now we load the LoRA and test:
text = tokenizer.apply_chat_template([
    {"role" : "system", "content" : SYSTEM_PROMPT},
    {"role" : "user", "content" : "南極で使う冷蔵庫の主な機能は何ですか?３つ挙げてください。"},
], tokenize = False, add_generation_prompt = True)

# 中略
output

```

**(reasoning)**

南極は世界で最も寒い地域の一つであり、そこでの冷蔵庫は通常の使用環境とは異なる特別な機能を備えています。以下は、南極で使用される冷蔵庫の主な機能についての考察です。

1. **高効率な断熱材の使用**: 南極の極寒の気候では、冷蔵庫内の温度を一定に保つために高効率な断熱材が必要です。これにより、冷蔵庫が外部の極端な低温に影響されることを防ぎ、電力消費を最小限に抑えることができます。

2. **過冷却防止機能**: 極寒の環境では、冷蔵庫の冷却システムが過冷却し、冷蔵庫内の食品やサンプルが凍結してしまう可能性があります。このため、過冷却を防ぎ、冷蔵庫内の温度を適切な範囲に保つための機能が重要です。

3. **バックアップ電源システム**: 南極の研究施設では、電力供給が途絶えるリスクがあるため、冷蔵庫にはバックアップ電源システムが搭載されていることが多いです。これにより、電力が途絶えた場合でも一定期間冷蔵庫が稼働し続け、貴重なサンプルや食品が損傷するのを防ぎます。

これらの機能は、南極のような過酷な環境で冷蔵庫が適切に機能し続けるために不可欠です。

**(/reasoning)**

**(answer)**

1. 高効率な断熱材の使用
2. 過冷却防止機能
3. バックアップ電源システム

**(/answer)**


D先生： “これは面白い質問ですね。一種の論理題とも言えます。それでも、GRPOモデルはうまく答えています。今回は、Reasoningがとてもうまく効いています。”

QEU:FOUNDER ： “そして、これ（↓）が最後の質問です。これには結構、驚きますよ。ドン！！”

```python
#################################
# INFERENCE(6)
#################################
# ---
# Now we load the LoRA and test:
text = tokenizer.apply_chat_template([
    {"role" : "system", "content" : SYSTEM_PROMPT},
    {"role" : "user", "content" : "アンリはリンゴを3個、ジャックは梨を4個、ジルは本を5冊持っています。ジャックの梨を5個食べたら、果物は全部で何個になったでしょう？"},
], tokenize = False, add_generation_prompt = True)

# 中略
output

```

**(reasoning)**

1. 最初にアンリが持っているリンゴの数を確認します。アンリはリンゴを3個持っています。
2. 次に、ジャックが持っている梨の数を確認します。ジャックは梨を4個持っています。
3. 問題文によると、ジャックが5個の梨を食べたとあります。しかし、ジャックは元々4個の梨しか持っていません。したがって、彼は持っている全ての梨（4個）を食べることになります。
4. ジャックが梨を4個食べた後、彼の梨の数は0個になります。
5. 現在の果物の総数を計算します。アンリのリンゴ3個に加えて、ジャックの梨は0個です。
6. したがって、果物の総数は3個（アンリのリンゴ）のみです。

**(/reasoning)**

**(answer)**
3個
**(/answer)**


D先生： “このREASONINGはうまい！！それにしても、GRPOは、**論理題に対しては感動的につよい**なあ・・・。”

QEU:FOUNDER ： “だから、今回の結論として「向き不向き」なんだよね。このモデルは・・・。少なくとも、小生としましては・・・。”

D先生： “あれ？この REASONINGモデルをBONSAIシステム（↓）に使うんですよね？まさか、「こんなに素晴らしいモデル」を使わないなんて・・・。”

![imageBSR1-7-6](/2025-02-20-QEUR23_BS4RVW6/imageBSR1-7-6.jpg) 

QEU:FOUNDER ： “BONSAIディベートのモデルには、構成チーム(A/B)に合わせた**「独自の意識づけ(preference)」を付加させたい**です。この場合には、絶対的な論理を身に着けるGRPOのやり方はむきません。”

D先生： “ただし、論理データ取り用には使えますね。ちょっと「歩留まり」が悪そうですが・・・（笑）。”

![imageBSR1-7-7](/2025-02-20-QEUR23_BS4RVW6/imageBSR1-7-7.jpg) 

QEU:FOUNDER ： “次に、我々はORPOに戻ります。歴史的な**超大天才の名言（↑）**に注目したい・・・。”

D先生：“あれ？ ORPOに戻るんですか？”

![imageBSR1-7-8](/2025-02-20-QEUR23_BS4RVW6/imageBSR1-7-8.jpg) 

QEU:FOUNDER ： “QEUシステムは変わりつつあります。次回に、つづきます。”



## ～ まとめ ～

D先生：“今現在、いわゆる AI業界でDeepSeekリリースという大事件が起こっている今日このごろ・・・。その一方で、Ｊ国の社会ではとても**「大きな地殻変動」が起こりつつあります**。”

[![MOVIE1](http://img.youtube.com/vi/cqEIXSrPiGI/0.jpg)](http://www.youtube.com/watch?v=cqEIXSrPiGI "兵庫県で起こっている問題ー公益通報制度について")

QEU:FOUNDER ： “**とんでもない「すごいこと」が起こっています**。それにしても、こんなローカルな話題が大規模になったことに驚いたわ・・・。”

D先生：“たぶんね・・・。ひそかに海外も注目しているからでしょう？”

[![MOVIE2](http://img.youtube.com/vi/XwKwWsevjCI/0.jpg)](http://www.youtube.com/watch?v=XwKwWsevjCI "How a Superstar Doomed a Japanese TV Station Overnight - The Fuji TV Scandal")

QEU:FOUNDER ： “えーっ！アンダーグラウンドとはいえ、海外でも、**すごい情報量**じゃないですか？”

C部長： “ははは・・・。J国って、**「外圧」に弱い**からなあ・・・。”

D先生： “さらに、こんなにマイナーな話（↓）が注目されています。”

[![MOVIE3](http://img.youtube.com/vi/2HhSCiMUpQ8/0.jpg)](http://www.youtube.com/watch?v=2HhSCiMUpQ8 "Japan Wants To Quit Its Toxic Work Culture: But Why Isn't It Working? | Insight")

C部長： “**若者が転職代行を使う**何ぞ、ホントにケシカラン。プンプン・・・。”

QEU:FOUNDER ： “それは、Ｃ部長が「雇用者」の側だからです。もし、小生が、今、若者であり、転職したいとなると、**絶対に（転職）代行を使う**わ・・・。”

D先生： “真ん中に第三者がいると、（若者の）転職が気に入らない**「前の会社が転職後の会社に、告げ口する」**などのトラブルが起きないですからね。最近のボスは、**度量が狭い**ですから。あっ、Ｃ部長の事ではないですよ・・・（笑）。”

C部長： “ぐぬぬ・・・。”

QEU:FOUNDER ： “海外の皆さまへ・・・。いまどきのJ国の若者の行動は、一見ヘンテコに見えても、**実はかなり合理的なのです**。意外と、（彼らは）優秀ですって・・・（笑）。”


