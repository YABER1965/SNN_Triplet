---
title: QEUR23_SMNRT43 – PyTorchでCNN-Tripletをやってみる(外観検査-その１)
date: 2025-01-17
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "NSOARTC", "データセット", "外観検査", "Vision language Model"]
excerpt: Siamese Networkをやってみる
---

## QEUR23_SMNRT43 – PyTorchでCNN-Tripletをやってみる(外観検査-その１)

## ～ 結論：CNNとSOARTCの併用がおススメ ～

### ・・・ 前回のつづきです ・・・

D先生 ： “なるほど・・・。TRIPLETとSIAMESEのEmbeddingの分布は似ていますね。それで、肝心のTRIPLETの判別精度はどうですか？”

[imageSMR2-43-1](/2025-01-17-QEUR23_SMNRT43/imageSMR2-43-1.jpg)

D先生 ： “ゲッ！**86%か・・・。**ずいぶんと悪い結果がでてしまった。なぜだろう・・・。TRIPLETの損失関数の改善でなんとかなりませんか？”

```python
##########################################
# LOSS FUNCTIONS
##########################################
# ---
class TripletLoss(nn.Module):
    """
    Triplet loss
    Takes embeddings of an anchor sample, a positive sample and a negative sample
    """
    def __init__(self, margin):
        super(TripletLoss, self).__init__()
        self.margin = margin

    def forward(self, anchor, positive, negative, size_average=True):
        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)
        #distance_positive_2p = (anchor - positive).pow(2).sum(1)  # .pow(.5)
        #distance_positive = torch.log(distance_positive_4p) - 0.5*torch.log(distance_positive_2p) 
        # ---
        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)
        #distance_negative_2p = (anchor - negative).pow(2).sum(1)  # .pow(.5)
        #distance_negative = torch.log(distance_negative_4p) - 0.5*torch.log(distance_negative_2p)

        #print(distance_positive)
        losses = F.relu(distance_positive - distance_negative + self.margin)
        return losses.mean() if size_average else losses.sum()

```

QEU:FOUNDER ： “いろいろやってみました。距離の次元数を挙げてみたり、対数に変換してみたり・・・。結果、ぜ～んぶ、失敗しました。”

D先生 ： “関数（↑）のコメント文に苦労の残骸が見えます（笑）。”

QEU:FOUNDER ： “いちど初心にもどって、ANCHORを平均画像ではなく、ランダムに選択された個別画像にもどしてみました。そして、これが結果です。”

**(EMBEDDING)**

[imageSMR2-43-2](/2025-01-17-QEUR23_SMNRT43/imageSMR2-43-2.jpg)

**(CONFUSION MATRIX)**

[imageSMR2-43-3](/2025-01-17-QEUR23_SMNRT43/imageSMR2-43-3.jpg)

D先生 ： “ここまで大きく構造を変えても、TRIPLETでは予測精度は変わらないんですね。**Euclidには検出能力がない**なあ・・・。”

QEU:FOUNDER ： “ここまでで得た教訓は、**「TRIPLETでは、（現状のところ）EUCLID距離を使うしかない。よって、TRIPLETには距離を改造する自由度がほとんどない」**ということです。だから、「画像のごく一部だけ異なる状態を検出する（外観検査の）場合」において、TRIPLETは、通常のSiamese Neural Networkを改造した場合よりもパフォーマンスがわるくなりました。”

D先生 ： “まあ、我々は距離にこだわる人たちなので、TRIPLETでこれ以上頑張る必要はないですね。”

QEU:FOUNDER ： “そうです。我々は、基本、ここで「ギブアップ」です。・・・でもね、ここで一旦立ち止まり、**Convolution Neural Network(CNN)**をやってみたいんです。”

D先生 ： “いままでは、通常のMLP（多層モデル：Multi-layer Perceptron）を使っていますよね。忘れちゃいました。なぜ、FOUNDERは、CNNを使わなかったんですか？”

QEU:FOUNDER ： “2つの理由があります。NSOARTC法による画像変換処理で、すでに畳み込みを使っていることです。あと、本プロジェクトで使う計算は、なるべく軽くしたいのです。いまでは、かなりパワーアップしたとはいえ、**「Raspberry Pi」では、まだ非力**でしょ？”

[imageSMR2-43-4](/2025-01-17-QEUR23_SMNRT43/imageSMR2-43-4.jpg)

D先生 ： “ラズパイの性能は、かなり上がっていますね。”

QEU:FOUNDER ： “CNNって、この程度のマシンじゃ、かなりキツイと思うよ。今回、実験して実感すればわかります。プログラムをドン！！主要な変更部分です。”

```python
##########################################
# DATASETS: SIAMESE(tRIPLET) MNIST
##########################################
from PIL import Image
from torch.utils.data import Dataset
from torch.utils.data.sampler import BatchSampler

# ---
class TripletMNIST_train(Dataset):
    def __init__(self, mnist_dataset, max_dataset, transform=None):
        # ---
        self.train_labels = mnist_dataset["label"].numpy()
        self.train_data = mnist_dataset["image"]
        self.transform = transform
        # ---
        set_label = list(set(self.train_labels))
        self.len_label = len(set_label)
        print("len_label: ", self.len_label)
        # ---
        self.max_dataset = max_dataset
        self.arr_idx_anchor = []
        self.arr_idx_positive = []
        self.arr_idx_negative = []
        
        # ここでmnist_datasetを属性として追加
        #self.mnist_dataset = mnist_dataset
        for i in range(self.max_dataset):
            anchor_label = np.random.choice(set_label+[0,0,0])
            tmp_idx_positive = np.where(self.train_labels == anchor_label)[0]
            tmp_idx_negative = np.where(self.train_labels != anchor_label)[0] 
            # ---
            idx_anchor = np.random.choice(tmp_idx_positive)
            idx_positive = np.random.choice(tmp_idx_positive)
            idx_negative = np.random.choice(tmp_idx_negative)
            # ---
            self.arr_idx_anchor.append(idx_anchor)
            self.arr_idx_positive.append(idx_positive)
            self.arr_idx_negative.append(idx_negative) 

    def __getitem__(self, index):
        img_anchor = self.train_data[self.arr_idx_anchor[index]].unsqueeze(0)
        img_positive = self.train_data[self.arr_idx_positive[index]].unsqueeze(0)
        img_negative = self.train_data[self.arr_idx_negative[index]].unsqueeze(0)
        x = (img_anchor, img_positive, img_negative)
        
        if self.transform:
            img_anchor = self.transform(img_anchor)
            img_positive = self.transform(img_positive)
            img_negative = self.transform(img_negative)
            x = (img_anchor, img_positive, img_negative)
        
        return x, []

    def __len__(self):
        return self.max_dataset  # ここでmnist_datasetの長さを返す

# ---
# Set up data loaders
torch_dataset = TripletMNIST_train(transformed_dataset, 70000) # Returns pairs of images and target same/different

```

QEU:FOUNDER ： “わかりにくいので、自分で解説です。画像に対して、**追加としてunsqueezeの処理**をします。これは、畳み込みをする場合には欠かせません。”

```python
img_anchor = self.train_data[self.arr_idx_anchor[index]].unsqueeze(0)
```

D先生 ： “なるほど、つぎはCNNモデルの設計にいきましょう。”

```python
# ---
class EmbeddingNet(nn.Module):
    def __init__(self):
        super(EmbeddingNet, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3, 3), stride=(1, 1), pad-ding=(1, 1))
        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))
        self.dropout1 = nn.Dropout2d(p=0.25)
        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), pad-ding=(1, 1))
        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))
        #self.dropout2 = nn.Dropout2d(p=0.5)
        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), pad-ding=(1, 1))

        # 特徴マップの最終的なサイズを計算
        self.fc_input_size = 64*9*9
        
        self.fc1 = nn.Linear(self.fc_input_size, 256)
        #self.dropout3 = nn.Dropout(p=0.2)
        self.fc2 = nn.Linear(256, 128)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool1(x)
        x = self.dropout1(x)
        x = F.relu(self.conv2(x))
        x = self.pool2(x)
        #x = self.dropout2(x)
        x = F.relu(self.conv3(x))
        x = x.view(-1, self.fc_input_size)  
        x = F.relu(self.fc1(x))
        #x = self.dropout3(x)
        x = F.relu(self.fc2(x))
        return x

    def get_embedding(self, x):
        return self.forward(x)

```

QEU:FOUNDER ： “これが、グレースケール（チャンネル数が1）の基本的なCNNモデルです。Embeddingのために、128次元の情報が出力されます。”

D先生 ： “CNNのパラメータ設定って、わかりにくいですよね。”

```python
# Set up data loaders
cuda = torch.cuda.is_available()
batch_size = 64
kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}
triplet_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuf-fle=True, **kwargs)
triplet_test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, **kwargs)

```

QEU:FOUNDER ： “トライ・エラーで、たまたまにうまく行ったので、このパラメータにしました。詳しい理屈は、全くわからないです。ただし、**モデルの64という数字はDataLoaderのバッチ数からきている**ことは記録しておきたいです。”

D先生 ： “今回は、TRIPLET_LOSSを変えたのですか？”

QEU:FOUNDER ： “それ（損失関数）を変えることは、少なくとも小生の力量ではできませんでした。だから、小生としては、**Euclid距離みたいに雑な尺度でも、ちゃんと異常を識別できる**ことを祈るだけです。それでは、Embeddingを出力してみましょう。”

```python
# ---
from sklearn.model_selection import train_test_split

# ---
torch_dataset = TensorDataset(transformed_dataset["image"], transformed_dataset["label"])
train_dataset, test_dataset = train_test_split(torch_dataset, test_size=0.3)

########################################
# CREATE DATALOADER
########################################
# ---
# Baseline: Classification with softmax
# Set up data loaders
kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, **kwargs)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, **kwargs)

```

QEU:FOUNDER ： “CNNモデルを動かすために、評価用のデータでも**「batch_size = 64」**にしておきました。”

D先生 ： “あとの評価プログラムは、以前コードと同じでしょ？”

```python
# ---
#setting model in evaluation mode.
model.eval()

# ---
##########################################
# RVALUATING BY EMBEDDING
##########################################
# ---
def extract_embeddings(dataloader, model):
    with torch.no_grad():
        model.eval()
        embeddings = np.zeros((len(dataloader.dataset), 128))
        labels = np.zeros(len(dataloader.dataset))
        k = 0
        for images, target in dataloader:
            images =  torch.unsqueeze(images,1)
            if cuda:
                images = images.cuda()
            embeddings[k:k+len(images)] = model.get_embedding(images.cuda()).data.cpu().numpy()
            labels[k:k+len(images)] = target.numpy()
            k += len(images)
    return embeddings, labels

#####################################
# EVALUATING RESULT
#####################################
# ---
ds_embeddings, ds_labels = extract_embeddings(test_loader, model)
ds_embeddings

```

QEU:FOUNDER ： “さきほどと同様に、**「images =  torch.unsqueeze(images,1)」という処理を追加する**必要があります。それでは、CNNモデルを使った場合のEmbeddingのばらつきの状態をみてみましょう。”

[imageSMR2-43-5](/2025-01-17-QEUR23_SMNRT43/imageSMR2-43-5.jpg)

D先生 ： “えっ！CNNって、こんなに識別能力が高いんですか？”

[imageSMR2-43-6](/2025-01-17-QEUR23_SMNRT43/imageSMR2-43-6.jpg)

QEU:FOUNDER ： “あらためてCNNモデルのもつ威力に驚きました。もちろん、CNNで生画像をそのまま学習させたくはないが、中間処理であるNSOA(RT)C処理を入力すれば、もっと精度がよくなる可能性は高いです。”

[imageSMR2-43-7](/2025-01-17-QEUR23_SMNRT43/imageSMR2-43-7.jpg)

D先生 ： “あとは、お次は「予測精度」を評価しましょう。それが終われば、Siamese Neural Netを使った外観検査自動機の開発は終了ですね。めでたく・・・。”


## ～ まとめ ～

### ・・・ 前回のつづきです ・・・

C部長 : “A国は、自動車会社(BIG3 etc.)に遠慮をしちゃって、長年にわたって、鉄道開発を軽視しており、そのツケが今来たんですよね。 その意味で、A国は賢いのか、それとも「アレ」なのか・・・。“

QEU:FOUNDER ： “それに引き換え、C国は**「しっかりと正しい道を歩いている」**と思わん？ちゃんとしているから、C国は、他の国に余計な迷惑を掛けないんですよ。”

[imageSMR2-43-8](/2025-01-17-QEUR23_SMNRT43/imageSMR2-43-8.jpg)

QEU:FOUNDER ： “ただし、C国のやり方は、**しっかりしているから「ジミ」**なんですよ。だからのネトウヨ様たちは、あまりC国のスタイルが好きじゃない。これが、嫌C人類の本質じゃないのか？”

[imageSMR2-43-9](/2025-01-17-QEUR23_SMNRT43/imageSMR2-43-9.jpg)

C部長 : “C国のやり方は、ダサいと映るんでしょう。それに比べて、Aは、いちいち派手です。あと、（投票による）民主の印象もあるでしょう。 “

[![MOVIE1](http://img.youtube.com/vi/yAvPxg9JgJI/0.jpg)](http://www.youtube.com/watch?v=yAvPxg9JgJI "そりゃ民主的にやったらダサくなるよ。独裁主義でやらなぁカッコいいもんは出来へん")

QEU:FOUNDER ： “投票の型式という角度からみると、C国は（比較的に）非民主ではあるよね。ただし、高い確率で、比較的にまともな人がでている。その一方で・・・。”

[imageSMR2-43-10](/2025-01-17-QEUR23_SMNRT43/imageSMR2-43-10.jpg)

C部長 : “J国では、民主的な手法で、こんな人（↑）がでてきますからね。 **犬猫野菜という、新しい民主用語も生まれました。**“

QEU:FOUNDER ： “難しいもんですね。Cさんは、この人（↓）好き？”

[imageSMR2-43-11](/2025-01-17-QEUR23_SMNRT43/imageSMR2-43-11.jpg)

C部長 : “もう、大好きですよ！“

QEU:FOUNDER ： “小生は、この人のほうがいいですがね。”

[imageSMR2-43-12](/2025-01-17-QEUR23_SMNRT43/imageSMR2-43-12.jpg)

C部長 : “えっ！？スゴイ地味・・・。“

QEU:FOUNDER ： “つまらない人のほうが好きなんです。”
