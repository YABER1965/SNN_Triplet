---
title: QEUR23_SMNRT40 – PyTorchでTripletをやってみる(MNIST-その１)
date: 2025-01-12
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "NSOARTC", "データセット", "外観検査", "Vision language Model"]
excerpt: Siamese Networkをやってみる
---

## QEUR23_SMNRT40 – PyTorchでTripletをやってみる(MNIST-その１)

## ～ まずは簡単なデータセットを使いましょう ～

QEU:FOUNDER ： “いよいよTRIPLETを使って、画像の判別をやってみましょう。PyTorchでやってみるということです。ここまで来るまでの道のりは、ホントに長かった・・・。”

![imageSMR2-40-1](/2025-01-12-QEUR23_SMNRT26/imageSMR2-40-1.jpg)

QEU:FOUNDER ： “Kerasですでに実験済みだが、TripletではSiameseよりも予測精度が良くなります。まずはウォーミング・アップです。簡単なMNISTデータセットで実験をしてみましょう。プログラムをドン！！ すでに何回も類似の実験をしているので、紹介するコードは極力に簡素化し、我々のコメントも少なめに・・・。”

```python
# ---
# Common setup
import torch
from torch.optim import lr_scheduler
import torch.optim as optim
from torch.autograd import Variable
# ---
import torch.nn as nn
import torch.nn.functional as F
# ---
import numpy as np
cuda = torch.cuda.is_available()
# ---
#%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
# ---
# Prepare dataset
from torchvision.datasets import MNIST
from torchvision import transforms

# ---
mean, std = 0.1307, 0.3081
train_dataset = MNIST('../data/MNIST', train=True, download=True,
                             transform=transforms.Compose([
                                 transforms.ToTensor(),
                                 transforms.Normalize((mean,), (std,))
                             ]))
test_dataset = MNIST('../data/MNIST', train=False, download=True,
                            transform=transforms.Compose([
                                transforms.ToTensor(),
                                transforms.Normalize((mean,), (std,))
                            ]))

################################
# DATASET PARAMETERS
################################
# ---
n_classes = 10
mnist_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',
              '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',
              '#bcbd22', '#17becf']

```

D先生 ： “機械学習の定番、手書き文字MNIST(created by YLC)ですね。”

```python
##################################
# NETWORKS FUNCTIONS (TRIPLET)
##################################
# ---
# build model
class EmbeddingNet(nn.Module):
    def __init__(self):
        super(EmbeddingNet,self).__init__()
        self.linear1 = nn.Linear(28*28, 256) #28 * 28 is the pixels for each image
        self.linear2 = nn.Linear(256, 256) #100 and 5o
        self.final = nn.Linear(256, 128)
        self.relu = nn.ReLU()
# ここでは、このクラスのインスタンスを定義しながら、各レイヤーにデータを入力し、出力を返すだけです。
    def forward(self, img): #convert + flatten
        x = img.view(-1, 28*28)
        x = self.relu(self.linear1(x))
        x = self.relu(self.linear2(x))
        x = self.final(x)
        return x

    def get_embedding(self, x):
        return self.forward(x)
		
```

QEU:FOUNDER ： “ここで、晒しを中断してコメントです。ここでは、**128次元のEMBEDDINGを出力させたいので、モデルのパラメータを128に調整しています**。もし、2次元のベクトルを直接に生成して、散布図を出力したいときには、この値を2にすればいいです。”

```python
class TripletNet(nn.Module):
    def __init__(self, embedding_net):
        super(TripletNet, self).__init__()
        self.embedding_net = embedding_net

    def forward(self, x1, x2, x3):
        output1 = self.embedding_net(x1)
        output2 = self.embedding_net(x2)
        output3 = self.embedding_net(x3)
        return output1, output2, output3

    def get_embedding(self, x):
        return self.embedding_net(x)

##########################################
# DATASETS: SIAMESE MNIST
##########################################
from PIL import Image
from torch.utils.data import Dataset
from torch.utils.data.sampler import BatchSampler

class TripletMNIST(Dataset):
    def __init__(self, mnist_dataset):
        self.mnist_dataset = mnist_dataset
        self.train = self.mnist_dataset.train
        self.transform = self.mnist_dataset.transform

        if self.train:
            self.train_labels = self.mnist_dataset.train_labels
            self.train_data = self.mnist_dataset.train_data
            self.labels_set = set(self.train_labels.numpy())
            self.label_to_indices = {label: np.where(self.train_labels.numpy() == label)[0]
                                     for label in self.labels_set}

        else:
            self.test_labels = self.mnist_dataset.test_labels
            self.test_data = self.mnist_dataset.test_data
            # generate fixed triplets for testing
            self.labels_set = set(self.test_labels.numpy())
            self.label_to_indices = {label: np.where(self.test_labels.numpy() == label)[0]
                                     for label in self.labels_set}

            random_state = np.random.RandomState(29)

            triplets = [[i,
                         random_state.choice(self.label_to_indices[self.test_labels[i].item()]),
                         random_state.choice(self.label_to_indices[
                                                 np.random.choice(
                                                     list(self.labels_set - set([self.test_labels[i].item()]))
                                                 )
                                             ])
                         ]
                        for i in range(len(self.test_data))]
            self.test_triplets = triplets

    def __getitem__(self, index):
        if self.train:
            img1, label1 = self.train_data[index], self.train_labels[index].item()
            positive_index = index
            while positive_index == index:
                positive_index = np.random.choice(self.label_to_indices[label1])
            negative_label = np.random.choice(list(self.labels_set - set([label1])))
            negative_index = np.random.choice(self.label_to_indices[negative_label])
            img2 = self.train_data[positive_index]
            img3 = self.train_data[negative_index]
        else:
            img1 = self.test_data[self.test_triplets[index][0]]
            img2 = self.test_data[self.test_triplets[index][1]]
            img3 = self.test_data[self.test_triplets[index][2]]

        img1 = Image.fromarray(img1.numpy(), mode='L')
        img2 = Image.fromarray(img2.numpy(), mode='L')
        img3 = Image.fromarray(img3.numpy(), mode='L')
        if self.transform is not None:
            img1 = self.transform(img1)
            img2 = self.transform(img2)
            img3 = self.transform(img3)
        return (img1, img2, img3), []

    def __len__(self):
        return len(self.mnist_dataset)

##########################################
# LOSS FUNCTIONS
##########################################
# ---
class TripletLoss(nn.Module):
    def __init__(self, margin):
        super(TripletLoss, self).__init__()
        self.margin = margin

    def forward(self, anchor, positive, negative, size_average=True):
        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)
        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)
        losses = F.relu(distance_positive - distance_negative + self.margin)
        return losses.mean() if size_average else losses.sum()

```

QEU:FOUNDER ： “損失関数の定義がポイントです。POSITIVE側の距離とNEGATIVE側の距離を組み合わせて、TRIPLET距離が生成されます。前回のSiamese 法ではデータセットの生成段階で距離を与えていますが、このTripletのプログラムではすべて学習プロセスの中に組み込まれています。”

```python
losses = F.relu(distance_positive - distance_negative + self.margin)
```

D先生 ： “このやり方は、**インプットされたデータ（anchor|positive|negative画像）**をEmbeddingに変換して、それらから損失を計算したんでしょう？”

```python
loss_fn = ContrastiveLoss(margin)
```

QEU:FOUNDER ： “たぶん・・・。必要ならば、このLOSS FUNCTIONの設計を変えてもいいと思います。前回SNNの事例ではContractive Lossの関数(↑)を使いました。それを改造すれば、Triplet用も作れるよ。それでは、プログラムの晒しをさらに続けましょう。”

```python
##################################
# TRAIN - FIT FUNCTIONS
##################################
# ---
def train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics):
    for metric in metrics:
        metric.reset()

    model.train()
    losses = []
    total_loss = 0

    for batch_idx, (data, target) in enumerate(train_loader):
        target = target if len(target) > 0 else None
        if not type(data) in (tuple, list):
            data = (data,)
        if cuda:
            data = tuple(d.cuda() for d in data)
            if target is not None:
                target = target.cuda()

        optimizer.zero_grad()
        outputs = model(*data)

        if type(outputs) not in (tuple, list):
            outputs = (outputs,)

        loss_inputs = outputs
        if target is not None:
            target = (target,)
            loss_inputs += target

        loss_outputs = loss_fn(*loss_inputs)
        loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs
        losses.append(loss.item())
        total_loss += loss.item()
        loss.backward()
        optimizer.step()

        for metric in metrics:
            metric(outputs, target, loss_outputs)

        if batch_idx % log_interval == 0:
            message = 'Train: [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                batch_idx * len(data[0]), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), np.mean(losses))
            for metric in metrics:
                message += '\t{}: {}'.format(metric.name(), metric.value())

            print(message)
            losses = []

    total_loss /= (batch_idx + 1)
    return total_loss, metrics

# ---
def test_epoch(val_loader, model, loss_fn, cuda, metrics):
    with torch.no_grad():
        for metric in metrics:
            metric.reset()
        model.eval()
        val_loss = 0
        for batch_idx, (data, target) in enumerate(val_loader):
            target = target if len(target) > 0 else None
            if not type(data) in (tuple, list):
                data = (data,)
            if cuda:
                data = tuple(d.cuda() for d in data)
                if target is not None:
                    target = target.cuda()

            outputs = model(*data)

            if type(outputs) not in (tuple, list):
                outputs = (outputs,)
            loss_inputs = outputs
            if target is not None:
                target = (target,)
                loss_inputs += target

            loss_outputs = loss_fn(*loss_inputs)
            loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs
            val_loss += loss.item()

            for metric in metrics:
                metric(outputs, target, loss_outputs)

    return val_loss, metrics

# ---
def fit(train_loader, val_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[],
        start_epoch=0):
    for epoch in range(0, start_epoch):
        scheduler.step()

    # ---
    arr_training_loss = []
    arr_validation_loss = []
    for epoch in range(start_epoch, n_epochs):
        scheduler.step()

        # Train stage
        train_loss, metrics = train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics)

        message = 'Epoch: {}/{}. Train set: Average loss: {:.4f}'.format(epoch + 1, n_epochs, train_loss)
        for metric in metrics:
            message += '\t{}: {}'.format(metric.name(), metric.value())

        val_loss, metrics = test_epoch(val_loader, model, loss_fn, cuda, metrics)
        val_loss /= len(val_loader)

        message += '\nEpoch: {}/{}. Validation set: Average loss: {:.4f}'.format(epoch + 1, n_epochs, val_loss)
        for metric in metrics:
            message += '\t{}: {}'.format(metric.name(), metric.value())

        # ---
        print(message)
        arr_training_loss.append(train_loss)
        arr_validation_loss.append(val_loss)
    
    return arr_training_loss, arr_validation_loss

##########################################
# MAIN : Triplet network
##########################################
# ---
# アンカー、正の例 (アンカーと同じクラス)、負の例 (アンカーとは異なるクラス) を受け取るトリプレット ネットワークをトレーニングします。
# Set up data loaders
triplet_train_dataset = TripletMNIST(train_dataset) # Returns triplets of images
triplet_test_dataset = TripletMNIST(test_dataset)
batch_size = 128
kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}
triplet_train_loader = torch.utils.data.DataLoader(triplet_train_dataset, batch_size=batch_size, shuffle=True, **kwargs)
triplet_test_loader = torch.utils.data.DataLoader(triplet_test_dataset, batch_size=batch_size, shuf-fle=False, **kwargs)

# ---
# Set up the network and training parameters
margin = 1.
embedding_net = EmbeddingNet()
model = TripletNet(embedding_net)
if cuda:
    model.cuda()
loss_fn = TripletLoss(margin)
lr = 2e-4
optimizer = optim.Adam(model.parameters(), lr=lr)
scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)
n_epochs = 10
log_interval = 100

##############################
# TRAINING RUN (TRIPLET SIAMESE)
##############################
# ---
arr_training_loss, arr_validation_loss = fit(triplet_train_loader, triplet_test_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval)
	
```

QEU:FOUNDER ： “あとは、この学習結果からEMBEDDINGを生成するわけです。EMBEDDING散布図を出力するためのプログラムの紹介はスキップします。”

![imageSMR2-40-2](/2025-01-12-QEUR23_SMNRT26/imageSMR2-40-2.jpg)

D先生 ： “やはり、**TRIPLETでは「分解能」が全然違います**。でも、実際に判別に使うのでしたら128次元になるんでしょう？”

QEU:FOUNDER ： “じゃあ、ご要望に応えて、パラメータを128に変更してみました。”

![imageSMR2-40-3](/2025-01-12-QEUR23_SMNRT26/imageSMR2-40-3.jpg)

D先生 ： “この結果も、もちろん素晴らしいです。・・・で？肝心の予測の正確度は？”

QEU:FOUNDER ： “プログラムの晒しに疲れちゃったので、次回につづきます。”


## ～ まとめ ～

QEU:FOUNDER ： “あ～あ・・・。こんなことになるなんて・・・。**総損失は何十兆円になる**んだ？”

![imageSMR2-40-4](/2025-01-12-QEUR23_SMNRT26/imageSMR2-40-4.jpg)

C部長 : “なんか、自分が推しているアイドルのスッピンを見てしまった気持ちですね。**J国の宗主国であるA国をモデルにすれば、必ず良い国になる！！**そのために、A国のサジェスチョン(↓)は大切！！・・・ですもんね。 “

![imageSMR2-40-5](/2025-01-12-QEUR23_SMNRT26/imageSMR2-40-5.jpg)

QEU:FOUNDER ： “そうそう・・・。とてもありがたいもんですよ。我々を導いてくれる、**「よき羊飼い」のA国様**でございます。そのA国様の、このような大災害を見てしまって、小生はとても悲しい・・・。それにしても、火って、なかなか消えてくれないのですね。**スゴイ爆弾はサクサク作れるのに、火を消すことができないんだ**。なんで？”

![imageSMR2-40-6](/2025-01-12-QEUR23_SMNRT26/imageSMR2-40-6.jpg)

C部長 : “なぜでしょうね。早期に、大量の水をまけば消火ができるはずです。水が手に入らなかったのかなあ・・・。 “

![imageSMR2-40-7](/2025-01-12-QEUR23_SMNRT26/imageSMR2-40-7.jpg)

QEU:FOUNDER ： “カリフォルニア発、prop27に関連するんだろうなあ・・・。”

C部長 : “prop27って、お菓子ですか？ “

QEU:FOUNDER ： “こんなモンです。ちょっと、信じられないでしょ？あの地域では、お店が少なくなりました。万引きが増えたんです。”

![imageSMR2-40-8](/2025-01-12-QEUR23_SMNRT26/imageSMR2-40-8.jpg)

QEU:FOUNDER ： “有名な某大統領候補が検事時代に、その設立に深く関与したので有名です。コストダウンが目的です。**究極の自助の世界を目指す**そうです。”

C部長 : “究極の自助の世界？ “

![imageSMR2-40-9](/2025-01-12-QEUR23_SMNRT26/imageSMR2-40-9.jpg)

QEU:FOUNDER ： “なんかの番組で見たなあ・・・。あのSTATEでは、庶民が住む街中にいる警官がとても少ないんです。しかし、高級住宅街ではたくさんのセキュリティーが見回っています。これは警官じゃないよ。自分たちで雇っているんです。**自分の金は、あくまで自分だけのために使う。税金で広く社会に分配なぞ、とんでもない！！**”

C部長 : “そのような近視眼的な発想で社会的インフラが決定的に弱体化したんですね。 “

![imageSMR2-40-10](/2025-01-12-QEUR23_SMNRT26/imageSMR2-40-10.jpg)

QEU:FOUNDER ： “ある種の人々の考え方が前面に出過ぎたんだね。あの地域では・・・。おそろしいことに、**そのSTATEがDEMOCRATSだ**という・・・。本当に、このような状況を「反面教師」にしていただきたいですね。”

![imageSMR2-40-11](/2025-01-12-QEUR23_SMNRT26/imageSMR2-40-11.jpg)

C部長 : “ああ・・・。あの人は、お金持ちだからなあ・・・。自分の事しか考えられないんだ。 “

[![MOVIE1](http://img.youtube.com/vi/hxiPszsOoxs/0.jpg)](http://www.youtube.com/watch?v=hxiPszsOoxs "トランプとその周辺のヤバさがバレ始めた模様")

QEU:FOUNDER ： “すくなくとも、この危機管理の大家(↓)は新年早々、間違いましたね。”

![imageSMR2-40-12](/2025-01-12-QEUR23_SMNRT26/imageSMR2-40-12.jpg)

QEU:FOUNDER ： “**今年の一番のリスクは「A国の自滅」なのではないでしょうか**。”

