---
title: QEUR23_SMNRT17 –  EMBEDDINGを使ったSNNモデルの評価
date: 2024-12-28
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "NSOARTC", "データセット", "外観検査", "Vision language Model"]
excerpt: Siamese Networkをやってみる
---

## QEUR23_SMNRT17 –  EMBEDDINGを使ったSNNモデルの評価

## ～ 「ユークリッド距離」を除けば、SNNは優秀！ ～

### ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “どれぐらいの正確度になると思う？”

D先生 ： “80％ぐらいじゃないですか？”

![imageSMR2-8-1](/2024-12-28-QEUR23_SMNRT17/imageSMR2-8-1.jpg)

D先生 ： “えーっ！？なんや？この精度(accuracy)は！？”

![imageSMR2-8-2](/2024-12-28-QEUR23_SMNRT17/imageSMR2-8-2.jpg)

QEU:FOUNDER ： “そもそも、この（正確には前ブログの）評価プログラムでは、Triplet一致性の評価をCOSIN類似度で評価しているでしょ？外観検査の場合には、COSIN類似度による判別結果がどこまで信頼できるのかという問題があるよね。それでは、Embeddingを使った評価をします。前回のプログラムで学習したモデルを使用して、Embeddingを作成します。プログラムをドン！！”

```python
####################################
# Kerasモデルを読み込み、推論する
####################################
# ---
# Preparing for Model Training/Evaluation
# Import all libraries
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
# ---
import keras
from keras import Input, optimizers, Model
from keras.layers import Layer, Dense, Lambda
from keras.ops import norm
from tensorflow.keras.optimizers import Adam
from keras.models import Sequential, Model
from keras.saving import register_keras_serializable
# ---
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.utils import plot_model
# ---
from scipy.spatial import distance

# ---
image_size = 1404
# ---
# Define the model
model = Sequential(
    [
        Input(shape=(image_size,)),
        Dense(512, activation="relu"),
        Dense(256, activation="relu"),
        Dense(128, activation=None),
    ]
)

# ---
# Register the custom function for serialization
@register_keras_serializable()
def euclidean_distance(twin1_output, twin2_output):
    """Compute the euclidean distance (norm) of the output of
    the twin networks.
    """
    return tf.sqrt(tf.reduce_sum(tf.square(twin1_output - twin2_output), axis=1, keepdims=True))

# ---
@register_keras_serializable()
def triplet_loss(templates, margin=0.4):
    
    anchor,positive,negative = templates
    positive_distance = euclidean_distance(anchor,positive)
    negative_distance = euclidean_distance(anchor,negative)

    basic_loss = positive_distance-negative_distance+margin
    loss = K.maximum(basic_loss,0.0)
    
    return loss

# ---
# Define inputs
anchor = Input(shape=(image_size,), name='anchor_input')
A = model(anchor)

positive = Input(shape=(image_size,), name='positive_input')
P = model(positive)

negative = Input(shape=(image_size,), name='negative_input')
N = model(negative)

# Compute distance using Lambda layer with output_shape specified
loss = Lambda(triplet_loss, output_shape=(1,))([A, P, N])
model = Model(inputs=[anchor,positive,negative],outputs=loss)

# ---
# グラウンドトゥルースラベルがないのでカスタム損失関数を作成する
# Define loss function for serialization
@register_keras_serializable()
def identity_loss(y_true, y_pred):
    return K.mean(y_pred)

model.compile(loss=identity_loss, optimizer=Adam(learning_rate=3e-5))

# ---
# モデルの呼び出し
from tensorflow.keras.models import load_model

model = load_model("drive/MyDrive/TANSHI_euclid_model.keras")

# ---
# Display the loaded model's architecture
model.summary()

# ----
# データセットを読み込む
# ----
# Importing the dataset
import numpy as np
import pandas as pd
from datasets import Dataset, load_dataset

# ---
####################
# データフレームdfから、学習に必要なXs,Ysを分解する関数
####################
# ---
def create_data(load_df):
    # ---
    X = load_df.loc[:,'data0':'data1403'].values
    #print(X)

    label = load_df.loc[:,'label'].values
    #print(label[0:20])

    fmode = load_df.loc[:,'fmode'].values
    #print(fmode[0:20])

    return X, label, fmode

# ---
dataset = load_dataset('YxBxRyXJx/NSOARTC_Tanshi_1221', split='train')
#print(datasets)
# ---
# データセットをPandasのデータフレームに変換
org_df = pd.DataFrame(dataset)  # 'train'分割を使用する場合
# データフレームの内容を表示
#print(org_df)
# ---
# 何種類のラベルがあるか？
labels = org_df.loc[:,'label'].values
set_label = list(set(labels))
print(set_label)

# ---
# データフレームの行をランダムにシャッフル
shuffled_df = org_df.sample(frac=1).reset_index(drop=True)
# シャッフルされたデータフレームの表示
#print(shuffled_df)

# ---
# X-Y分解(シャッフル)
shuffled_X, shuffled_label, shuffled_fmode = create_data(shuffled_df)
print(shuffled_X.shape)

#########################################
# ANCHORのベクトル群を生成する
# TARGET = 以下のラベルのデータ群の平均値ベクトルとなる
# label: [0, 11, 13, 21, 23, 31, 32, 33, 42, 61, 62, 63, 71, 72, 73, 81, 82, 91, 92]
#########################################
# ---
def create_anchors(load_df, TARGET):
    # ---
    # TARGETを抽出する
    cut_df = load_df.loc[load_df.loc[:,'label']==TARGET,:]
    #print(cut_df)
    # ---
    # X-Y分解する
    cut_X, cut_label, cut_fmode = create_data(cut_df)
    # ---
    # anchorを生成する
    anchor_X = []
    for j in range(cut_X.shape[1]):
        anchor_X.append(np.mean(cut_X[:,j]))

    return cut_X.shape[0], np.array(anchor_X)

# ---
anchor_Xs = []
arr_Xnum  = [] 
for TARGET in set_label:
    num_Xs, arr_anchor = create_anchors(org_df, TARGET)
    anchor_Xs.append(arr_anchor)
    arr_Xnum.append(num_Xs)
# ---
anchor_Xs = np.array(anchor_Xs)
arr_Xnum = np.array(arr_Xnum)
print(anchor_Xs.shape)
print("-------")
print(arr_Xnum)

################################
# ANCHORを生成する
################################
# ---
# extract the CNN model for inference
siamese_model = model.layers[3]

#########################################
# ANCHORのベクトル群を生成する
# TARGET = 以下のラベルのデータ群の平均値ベクトルとなる
# label: [0, 11, 13, 21, 23, 31, 32, 33, 42, 61, 62, 63, 71, 72, 73, 81, 82, 91, 92]
#########################################
# ---
def create_anchors(load_df, TARGET):
    # ---
    # TARGETを抽出する
    cut_df = load_df.loc[load_df.loc[:,'label']==TARGET,:]
    #print(cut_df)
    # ---
    # X-Y分解する
    cut_X, cut_label, cut_fmode = create_data(cut_df)
    # ---
    # anchorを生成する
    anchor_X = []
    for j in range(cut_X.shape[1]):
        anchor_X.append(np.mean(cut_X[:,j]))

    return cut_X.shape[0], np.array(anchor_X)

# ---
mx_anchor = []
arr_Xnum  = [] 
for TARGET in set_label:
    num_Xs, arr_anchor = create_anchors(org_df, TARGET)
    mx_anchor.append(arr_anchor)
    arr_Xnum.append(num_Xs)
# ---
mx_anchor = np.array(mx_anchor)
arr_Xnum = np.array(arr_Xnum)
print(mx_anchor.shape)
#print("-------")
#print(arr_Xnum)

################################
# anchor_embeddingを生成する
################################
# ---
anchor_embedding = []
for i in range(len(arr_Xnum)):
    # ---
    X_embedding = siamese_model.predict(np.array([mx_anchor[i]]))[0].tolist()
    anchor_embedding.append(X_embedding)
    #print(X_test_embedding)
# ---
anchor_embedding = np.array(anchor_embedding)
print(anchor_embedding.shape)

################################
# ユークリッド距離を計算する(1.個別)
################################
# ---
# ひとつのデータを抽出する
def individual_process(i, shuffled_X, shuffled_label, shuffled_fmode, anchor_embedding, mx_anchor):
    arr_measure = np.array([shuffled_X[i]])
    sample_measure = siamese_model.predict(arr_measure)[0]
    # ---
    str_label = shuffled_label[i]
    str_fmode = shuffled_fmode[i]
    # ---
    arr_dist = []
    for k, sample_anchor in enumerate(anchor_embedding):
        # ---
        #sample_anchor = anchor_embedding[k]
        val_dist = distance.euclidean(sample_measure, sample_anchor)
        arr_dist.append(round(val_dist,4))
    # ---
    print("--- ACTUAL DATA ---")
    print(f"iCount: {i}, Label: {str_label}, Mode: {str_fmode}")
    print("--- set_label ---")
    print(set_label)
    print("--- arr_dist ---")
    print(arr_dist)
    print("--- MIN_INDEX ---")
    index = np.argmin(arr_dist)
    print(f"INDEX: {index}, LABEL: {set_label[index]}")
    # ---
    fig = plt.figure(figsize=[8,10])
    plt.title('  MEASUREMENT  |  ANCHOR  ',fontsize = 15)
    plt.tight_layout()
    plt.axis('off')
    # ---
    fig.add_subplot(2, 2, 1)
    plt.imshow(shuffled_X[i].reshape(36,39), cmap='hot', interpolation='nearest')
    # ---
    fig.add_subplot(2, 2, 2)
    plt.imshow(mx_anchor[index].reshape(36,39), cmap='hot', interpolation='nearest')
    # ---
    # ヒートマップを表示
    plt.show()

# ---
# ひとつのデータを抽出する
i = 0
individual_process(i, shuffled_X, shuffled_label, shuffled_fmode, anchor_embedding, mx_anchor)

```

D先生 ： “まずは、個別に１つの推論の結果を見たいということですね。やった！大正解ですね！”

![imageSMR2-8-3](/2024-12-28-QEUR23_SMNRT17/imageSMR2-8-3.jpg)

QEU:FOUNDER ： “今回のロジックは、19種類のアンカーから引き出されたEmbeddingベクトル群と計測対象画像のEmbeddingベクトルをEuclid距離で比較したんです。当たり前ながら、**Euclid距離が最小になるアンカーが、推論の結果になります**。この場合には、「42」に相当するベクトルの距離が最小になりました。ついでに、もう2件の結果をみてみましょう。”

**（推論結果、その２）**

![imageSMR2-8-4](/2024-12-28-QEUR23_SMNRT17/imageSMR2-8-4.jpg)

**（推論結果、その３）**

![imageSMR2-8-5](/2024-12-28-QEUR23_SMNRT17/imageSMR2-8-5.jpg)

D先生 ： “これは面白い！不正解の事例が見つかりました。本来が合格品であるものが、不合格と判定されたようです。距離を確認すると、距離値が1.8付近の要素が2つあります。”

QEU:FOUNDER ： “偶然ながら、よい「失敗事例」を収集することができました。それでは、より多くの計測対象群に対して推論を行って、モデルの評価を行いましょう。プログラムをドン！！”

```python
################################
# ユークリッド距離を計算する(2.バッチ)
################################
# ---
# バッチ・データを抽出する
def batch_process(i, shuffled_X, shuffled_label, shuffled_fmode, anchor_embedding, mx_anchor):
    arr_measure = np.array([shuffled_X[i]])
    sample_measure = siamese_model.predict(arr_measure)[0]
    # ---
    str_label = shuffled_label[i]
    str_fmode = shuffled_fmode[i]
    # ---
    arr_dist = []
    for k, sample_anchor in enumerate(anchor_embedding):
        # ---
        #sample_anchor = anchor_embedding[k]
        val_dist = distance.euclidean(sample_measure, sample_anchor)
        arr_dist.append(round(val_dist,4))
    # ---
    index = np.argmin(arr_dist)
    #print(f"INDEX: {index}, LABEL: {set_label[index]}")

    return set_label[index], str_label

# ---
# バッチ・データを抽出するを抽出する
arr_pred = []
arr_actual = []
for i in range(500):
    pred_y, actual_y = batch_process(i, shuffled_X, shuffled_label, shuffled_fmode, anchor_embedding, mx_anchor)
    #print(f"i: {i}, pred: {pred_y}, actual: {actual_y}")
    arr_pred.append(pred_y)
    arr_actual.append(actual_y)

################################
# CREATING CONFUSION MATRIX
################################
# ---
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
#import numpy as np

# 混同行列の生成
cm = confusion_matrix(arr_actual, arr_pred)

# Confusion matrixの表示
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)

# X軸ラベル、Y軸ラベル、タイトルの追加
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')

# Confusion matrixの表示
plt.show()

################################
# Accuracy (正解率) : 正解の数/全データ
# Accuracy = (TP+TN) / (TP+TN+FP+FN)
################################
# ---
from sklearn.metrics import accuracy_score

# accuracy
accuracy = round(accuracy_score(arr_actual, arr_pred),5)
print(f"ACCURACY: {accuracy}")

```

D先生 ： “やったー！！かなりのハイスコアだ！！**93％の精度(Accuracy)がある**んですね。”

![imageSMR2-8-6](/2024-12-28-QEUR23_SMNRT17/imageSMR2-8-6.jpg)

QEU:FOUNDER ： “正解率がある程度たかいものの、失敗のパターンが決まっているんですよ。具体的に言えば、先ほど紹介した「合格品-不良品」の組み合わせです。**「不良品-不良品(別モード)」というパターンが非常に少ない**んです。”

D先生 ： “ああ、そうか・・・。Euclid距離をSNNの学習に使ったのでは、**「分解能」が足らなかった**のか・・・。”

![imageSMR2-8-7](/2024-12-28-QEUR23_SMNRT17/imageSMR2-8-7.jpg)

QEU:FOUNDER  ： “**より「高次の」距離を使う必要があるんです。**今回は、我々の読み通り、理想的な結果に終わりましたね（笑）。”


## ～ まとめ ～

## ・・・ 前回のつづきです ・・・

QEU:FOUNDER ： “特別会計は人々の目にふれないのだが、一般会計の場合には人々の間に触れてしまいます。つまり、一定レベルの国民の支持がないと、（国家予算をお友達に優先的に流すことが）出来ないんですよ。”

![imageSMR2-8-8](/2024-12-28-QEUR23_SMNRT17/imageSMR2-8-8.jpg)

D先生: “**「あの会社が儲かっている。スゴイ！！だから、J国スゴイ」**という、合理的な根拠のない「お気持ち」の作用がないと、「一般会計のソ連化」は不可能ですね。”

![imageSMR2-8-9](/2024-12-28-QEUR23_SMNRT17/imageSMR2-8-9.jpg)

QEU:FOUNDER ： “**「パワハラ、ミソジニー、お気持ち・・・」**と・・・。J国が世界にリードする分野があってよかったですね。”

[![MOVIE1](http://img.youtube.com/vi/WnjDBLrV1jY/0.jpg)](http://www.youtube.com/watch?v=WnjDBLrV1jY "「お気持ち帝国」大ニッポン90年の没落")

QEU:FOUNDER ： “「身を切るカイカク」で税の減らすことを目指す一方で、自分たちが旗を振る万博では**「お友達には大盤振る舞い」**しています。**彼らは半分左翼、「下半身がソ連的」**ですね。”

![imageSMR2-8-10](/2024-12-28-QEUR23_SMNRT17/imageSMR2-8-10.jpg)

D先生 : “著名な建築家が、この点を別の視点から論じていますね。”

[![MOVIE2](http://img.youtube.com/vi/EjHVrsCoBT4/0.jpg)](http://www.youtube.com/watch?v=EjHVrsCoBT4 "建築家山本理顕さん登場！大阪万博、責任者はだれだ！（西谷文和さん）【The Burning Issues】20241218")

QEU:FOUNDER ： “この場合には、**「経済のソ連化」**というよりも**「社会のソ連化」**ですね。とくに、ある地域（↓）においてはね。”

![imageSMR2-8-11](/2024-12-28-QEUR23_SMNRT17/imageSMR2-8-11.jpg)

C部長 : “あ～あ・・・。言っちゃった・・・。“

![imageSMR2-8-12](/2024-12-28-QEUR23_SMNRT17/imageSMR2-8-12.jpg)

QEU:FOUNDER ： “年末になって、大きな動きがありましたね。”

C部長 : “おそらく海外の方が、よくわかっているんですよ。あそこで、一体。何が起こっているのか・・・。・・・でも、来年は、どうなるのかなあ？”

![imageSMR2-8-13](/2024-12-28-QEUR23_SMNRT17/imageSMR2-8-13.jpg)

QEU:FOUNDER ： “ここで予言します。絶対、**このイベントが失敗した原因は、国民のモラルが低下したからだ**と言い始めます。”

