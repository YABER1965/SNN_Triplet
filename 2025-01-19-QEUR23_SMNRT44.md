---
title: QEUR23_SMNRT44 – PyTorchでCNN-Tripletをやってみる(外観検査-その2)
date: 2025-01-19
tags: ["QEUシステム", "メトリックス", "Python言語", "Unsloth", "NSOARTC", "データセット", "外観検査", "Vision language Model"]
excerpt: Siamese Networkをやってみる
---

## QEUR23_SMNRT44 – PyTorchでCNN-Tripletをやってみる(外観検査-その2)

## ～ CNNの威力を痛感 ～

D先生 ： “今回は、前回に引き続き、TRIPLET型式のSiamese Neural Networkの予測性能の評価です。外観検査の欠陥検出精度を評価します。”

QEU:FOUNDER ： “今回は、なにもいうことはないです。プログラムをドン！！”

```python
# ---
# GAIKAN visual inspection recognizer with pytorch
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
from torchvision import datasets, transforms
# ---
import pandas as pd
from datasets import Dataset, load_dataset
import matplotlib
import numpy as np
import matplotlib.pyplot as plt

# ---
# Munging the data
# データをロードして、トレーニング データ (トレーニング セットと検証セットに分割) の ndarray を生成する必要があります。これは、PyTorch テンソルに変換できます。入力は 4d で、次元は 1) サンプル サイズ、2) チャネル数、3) 幅、4) 高さに対応します。

# ---
dataset = load_dataset('YxBxRyXJx/NSOARTC_Tanshi_1221', split='train')
#print(datasets)
# ---
# データセットをPandasのデータフレームに変換
df = pd.DataFrame(dataset)  # 'train'分割を使用する場合
#df.head()

# ---
def cube_data(load_df):
    """Creates an n-d array from data frame"""
    # ---
    imgs = load_df.loc[:,'data0':'data1403'].values
    labels = load_df.loc[:,'label'].values
    fmodes = load_df.loc[:,'fmode'].values
    # ---
    ntot = imgs.shape[0]         # number of rows
    # create a ntrain * 36 * 39 ndarray of training set
    cubes = np.ndarray(shape = (ntot, 1, 36, 39), dtype = "float")
    for i in range(0, ntot):
        cubes[i, 0, :, :] = imgs[i, :].reshape(36, 39)
    return cubes, labels, fmodes

# ---
cubes, labels, fmodes = cube_data(df)
set_label = list(set(labels))
print(set_label)
# ---
ntot = cubes.shape[0]
print(ntot)

# ---
def create_array(arr_index, labels, fmodes):
    # ---
    set_label = list(set(labels))
    arr_labels = []
    arr_fmodes = []
    for i in arr_index:
        idx_label = set_label.index(labels[i])
        arr_labels.append(idx_label)
        arr_fmodes.append(fmodes[i])
    return arr_labels, arr_fmodes

# --- 
# Split the data into a 60/40 training and validation set.
ntrain = np.floor(ntot * .6)
train_indices = np.random.randint(0, int(ntot), size = int(ntrain))
train_labels, train_fmodes = create_array(train_indices, labels, fmodes)
print(train_indices)

# ---
nvalid = ntot - ntrain
valid_indices = np.setdiff1d(list(range(0, ntot)), train_indices)
valid_labels, valid_fmodes = create_array(valid_indices, labels, fmodes)
print(valid_indices)

# ---
# break data into training and validation sets
train_cube = torch.FloatTensor(cubes[train_indices, :, :, :])
valid_cube = torch.FloatTensor(cubes[valid_indices, :, :, :])

train_labels = torch.from_numpy(np.array(train_labels))
valid_labels = torch.from_numpy(np.array(valid_labels))

# ---
# Creating data loaders
# pytorch のデータ ユーティリティを使用して、トレーニング データとテスト データのローダーを作成します。
trainset = torch.utils.data.TensorDataset(train_cube, train_labels)
train_loader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True, num_workers = 4)

testset = torch.utils.data.TensorDataset(valid_cube, valid_labels)
test_loader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True, num_workers = 4)

# ---
# Visualize some training examples
def imshow(img):
    img = img # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1,2,0)))

dataiter = iter(train_loader)
images, labels = next(dataiter)

imshow(torchvision.utils.make_grid(images))
print(' '.join('%5s'%int(labels.numpy()[j]) for j in range(train_loader.batch_size)))

```

D先生 ： “新しい形式で、画像を出力しましたね。画像の数は8x8=64ですか・・・。”

![imageSMR2-44-1](/2025-01-19-QEUR23_SMNRT44/imageSMR2-44-1.jpg)

QEU:FOUNDER ： “DataLoaderで作成したバッチ内の情報を出してくれたんです。それでは、コードの晒しを続けます。”

```python
# ---
class EmbeddingNet(nn.Module):
    def __init__(self):
        super(EmbeddingNet, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3, 3), stride=(1, 1), pad-ding=(1, 1))
        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))
        self.dropout1 = nn.Dropout2d(p=0.25)
        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), pad-ding=(1, 1))
        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))
        #self.dropout2 = nn.Dropout2d(p=0.5)
        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), pad-ding=(1, 1))

        # 特徴マップの最終的なサイズを計算
        self.fc_input_size = 64*9*9
        
        self.fc1 = nn.Linear(self.fc_input_size, 256)
        #self.dropout3 = nn.Dropout(p=0.2)
        self.fc2 = nn.Linear(256, 128)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool1(x)
        x = self.dropout1(x)
        x = F.relu(self.conv2(x))
        x = self.pool2(x)
        #x = self.dropout2(x)
        x = F.relu(self.conv3(x))
        x = x.view(-1, self.fc_input_size)  
        x = F.relu(self.fc1(x))
        #x = self.dropout3(x)
        x = F.relu(self.fc2(x))
        return x

    def get_embedding(self, x):
        return self.forward(x)

# ---
class TripletNet(nn.Module):
    def __init__(self, embedding_net):
        super(TripletNet, self).__init__()
        self.embedding_net = embedding_net

    def forward(self, x1, x2, x3):
        output1 = self.embedding_net(x1)
        output2 = self.embedding_net(x2)
        output3 = self.embedding_net(x3) 
        return output1, output2, output3

    def get_embedding(self, x):
        return self.embedding_net(x)

################################
# MODEL INSTANCE SETTING
################################
# ---
# Set up the network and training parameters
# ---
embedding_net = EmbeddingNet()
model = TripletNet(embedding_net)
cuda = torch.cuda.is_available()
if cuda:
    model.cuda()

# ----
# モデル定義が必要
model.load_state_dict(torch.load('drive/MyDrive/siamese_gaikan_triplet128_model.pth'))
model.eval()

# ---
##########################################
# EVALUATING BY EMBEDDING
##########################################
# ---
def extract_embeddings(dataloader, model):
    with torch.no_grad():
        model.eval()
        embeddings = np.zeros((len(dataloader.dataset), 128))
        labels = np.zeros(len(dataloader.dataset))
        k = 0
        for images, target in dataloader:
            #images =  torch.unsqueeze(images,1)
            if cuda:
                images = images.cuda()
            embeddings[k:k+len(images)] = model.get_embedding(images.cuda()).data.cpu().numpy()
            labels[k:k+len(images)] = target.numpy()
            k += len(images)
    return embeddings, labels

#####################################
# EVALUATING RESULT
#####################################
# ---
ds_embeddings, ds_labels = extract_embeddings(test_loader, model)
#ds_embeddings

# ---
from sklearn.decomposition import PCA
# PCAの実行
pca = PCA()
X_pca = pca.fit_transform(ds_embeddings)
# 主成分が説明する分散の割合を表示
#print(pca.explained_variance_ratio_)

# ---
set_label = list(set(labels.numpy()))
n_classes = len(set_label)
mnist_classes = set_label
colors = [
    '#1f77b4',  # 青
    '#ff7f0e',  # オレンジ
    '#2ca02c',  # 緑
    '#d62728',  # 赤
    '#9467bd',  # 紫
    '#8c564b',  # 茶色
    '#e377c2',  # ピンク
    '#7f7f7f',  # グレー
    '#bcbd22',  # 黄緑
    '#17becf',  # シアン
    '#ffbb78',  # 明るいオレンジ
    '#98df8a',  # 明るい緑
    '#ff9896',  # 明るい赤
    '#c5b0d5',  # 明るい紫
    '#c49c94',  # 明るい茶色
    '#f7b6d2',  # 明るいピンク
    '#dbdb8d',  # 明るい黄緑
    '#9edae5',  # 明るいシアン
    '#f5b041',  # 黄金色
    '#e67e22',  # テラコッタ
    '#3498db'   # スカイブルー
]

# ---
# PCA散布図をプロットする
def plot_embeddings(embeddings, targets, mnist_classes):
    plt.figure(figsize=(8,5))
    for i in range(len(mnist_classes)):
        inds = np.where(targets==i)[0]
        plt.scatter(embeddings[inds,0], embeddings[inds,1], alpha=0.5, color=colors[i])
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.title('2D Scatter Plot of the First Two Principal Components')
    plt.legend(mnist_classes)
    plt.show()

# ---
# グラフを生成する
plot_embeddings(X_pca, ds_labels, mnist_classes)

```

D先生 ： “前回の結果（Embeddingの散布図）と同じじゃないですか。”

![imageSMR2-44-2](/2025-01-19-QEUR23_SMNRT44/imageSMR2-44-2.jpg)

QEU:FOUNDER ： “もちろん、前回と同じGOOD結果です。この分布を見て、予測の新しいアプローチを考えました。クラス毎のEmbeddingのばらつきが小さいので、**予測で使うアンカー(ANCHOR)ベクトルをEmbeddingの平均値**にしようと思います。”

D先生 ： “いままでのやり方では、平均画像を最初に計算し、それをEmbeddingに変換しましたからね。それでは、次スニペットに進めましょう。いよいよ予測に進みます。”

```python
#############################
# TESTデータを使って、予測精度を検証します
#############################
# ---
#dataset = load_dataset('YxBxRyXJx/NSOARTC_Tanshi_1221', split='train')
dataset = load_dataset('YxBxRyXJx/NSOARTC_Tanshi_TEST_0107', split='train')
#print(datasets)
# ---
# データセットをPandasのデータフレームに変換
eval_df = pd.DataFrame(dataset)  # 'train'分割を使用する場合
# データフレームの内容を表示
#print(eval_df)

# ---
# Predicting labels for the test data
eval_cube, eval_labels, eval_fmodes = cube_data(eval_df)
neval = eval_cube.shape[0]
print(neval)

# ---
# break data into training and validation sets
eval_cube = torch.FloatTensor(eval_cube)
eval_labels = torch.from_numpy(np.array(eval_labels))


#####################################
# EVALUATING RESULT
#####################################
# ---
# Creating data loaders
# pytorch のデータ ユーティリティを使用して、トレーニング データとテスト データのローダーを作成します。
evalset = torch.utils.data.TensorDataset(eval_cube, eval_labels)
eval_loader = torch.utils.data.DataLoader(evalset, batch_size = 64, shuffle = True, num_workers = 4)

# ---
ds_embeddings, ds_labels = extract_embeddings(eval_loader, model)
#ds_embeddings

#######################
# CREATE ANCHOR MATRIX
#######################
# ---
def calc_average(embeddings):
    arr_average = np.zeros(128)
    for k in range(128):
        arr_average[k] = np.mean(embeddings[:,k])
    return arr_average

# ---
set_label = list(set(ds_labels))
mx_anchors = np.zeros([n_classes, 128])
len_embedding = len(ds_labels)
for i, nlabel in enumerate(set_label):
    mx_summary = []
    for j in range(len_embedding):
        if ds_labels[j] == nlabel:
            mx_summary.append(ds_embeddings[j].tolist())
    mx_summary = np.array(mx_summary)
    print(f"--- {i} ---")
    print(mx_summary)
    arr_average = calc_average(mx_summary)
    mx_anchors[i,:] = arr_average
# ---
print(mx_anchors)

```

QEU:FOUNDER ： “この部分で、クラス毎のEmbedding平均を計算しました。”

![imageSMR2-44-3](/2025-01-19-QEUR23_SMNRT44/imageSMR2-44-3.jpg)

D先生 ： “了解です。それでは、結論である、**Confusion Matrix**の算出に進みましょう。”

```python
# ---
from scipy.spatial import distance

# ---
anchor_embedding = mx_anchors
measure_embedding = ds_embeddings

################################
# ユークリッド距離を計算する
################################
# ---
# バッチ・データを抽出する
def calc_euclid(arr_measure, anchor_embedding):
    # ---
    arr_dist = []
    for j in range(n_classes):
        val_dist = distance.euclidean(anchor_embedding[j], arr_measure)
        arr_dist.append(round(val_dist,5))
        #print(f"j: {j}, Distance: {val_dist}")
    # ---
    #print("--- ACTUAL DATA, Distance ---")
    #print(f"iCount: {i}, arr_dist: {arr_dist}")
    index = np.argmin(arr_dist)

    return index

# ---
# バッチ・データを抽出するを抽出する
arr_pred = []
arr_actual = []
for i in range(1000):
    arr_measure = measure_embedding[i, :]
    index = calc_euclid(arr_measure, anchor_embedding)
    #print(f"i: {i}, pred: {pred_y}, actual: {actual_y}")
    arr_pred.append(int(set_label[index]))
    arr_actual.append(int(ds_labels[i]))
# ---
#print(arr_pred)
#print(arr_actual)

################################
# CREATING CONFUSION MATRIX
################################
# ---
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# 混同行列の生成
cm = confusion_matrix(arr_actual, arr_pred)

# Confusion matrixの表示
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues)

# X軸ラベル、Y軸ラベル、タイトルの追加
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')

# Confusion matrixの表示
plt.show()

```

QEU:FOUNDER ： “すごい予測のパワーでしょ？**「CNNモデル+TRIPLET」**って・・・。”

![imageSMR2-44-4](/2025-01-19-QEUR23_SMNRT44/imageSMR2-44-4.jpg)

D先生 ： “いままでの手法とは、まさに「別次元のレベル」です。その強さの背景については、すでに解説しました。あとは、FOUNDER・・・、ほかに言うことがありますかね・・・？”

QEU:FOUNDER ： “まとめは、「まとめ」で・・・（笑）。”


## ～ まとめ ～

### ・・・ 久々に、技術的なまとめをしましょう ・・・

C部長 : “この「まとめ」のスペースは、本来は技術的なまとめをするために設置されています。久々に有効活用できます。“

QEU:FOUNDER ： “ん！？いままでは有効活用されていないと・・・？失礼な・・・（笑）。今回のプロジェクトをまとめると、「どれを選ぶ？（↓）」という**樹形図**になるのかな？ちなみに、ここでいう**「Vanilla」**とは「（工夫のない）素うどん」という意味です。”

![imageSMR2-44-5](/2025-01-19-QEUR23_SMNRT44/imageSMR2-44-5.jpg)

C部長 : “これは、わかりやすい。「1.Vanilla」は、外観検査シーズとしては選択肢にあるのかな？ “

QEU:FOUNDER ： “いくらNSOARTCメトリックスを工夫しても、「1.Vanilla」では判別精度が40％も出ないのではないでしょうか。その意味で、実用上、まともに使えるのは、SiameseかTripletです。面白いのは、「TripletでもVanillaはダメダメ」ということです。”

C部長 : “「TripletかつVanilla」の実験結果に、ボクもびっくりしました。他の水準であれば、そこそこに使えるでしょうね。**「2.Siamese」は、意外と予測性能が高い**のですね。“

![imageSMR2-44-6](/2025-01-19-QEUR23_SMNRT44/imageSMR2-44-6.jpg)

QEU:FOUNDER ： “Siameseでは、**損失関数の距離を外部から与える--ことができるからね。あと、手法の選択基準は、**「ハード制約次第」**でしょうね。Vanillaであれば、普通のMLPモデルを使えば、ラズパイ上で動くことが可能です。”

C部長 : “そうねえ・・・。それでも、CNNの判別精度レベルの高さは魅力だなあ・・・。そうだ！**検査プロセス別に使い分け**ましょう。“

![imageSMR2-44-7](/2025-01-19-QEUR23_SMNRT44/imageSMR2-44-7.jpg)

QEU:FOUNDER ： “外観検査って、いろいろありますからね。**「中間検査か？最終検査か？」**で使い分けをするのが、もっとも「よさげ」ですね。”

![imageSMR2-44-8](/2025-01-19-QEUR23_SMNRT44/imageSMR2-44-8.jpg)

C部長 : “どのみち、**（外観）検査のデジタル化**は不可避です。もちろん、この技術は異常管理全般に使えます。“

![imageSMR2-44-9](/2025-01-19-QEUR23_SMNRT44/imageSMR2-44-9.jpg)

QEU:FOUNDER ： “さすがに、いまどきになって「こんなベタな外観検査（↑）」を人手でやっている会社はないと思うが・・・。”

C部長 ： “コネクタの端子検査レベルの難易度をもつ仕事は、まだ（人手による外観検査は）あるでしょう。”

![imageSMR2-44-10](/2025-01-19-QEUR23_SMNRT44/imageSMR2-44-10.jpg)

QEU:FOUNDER ： “NSOARTCメトリックスによる変換は、それなりに計算時間がかかるが、CPUやリソース制約への影響は緩いです。もちろん、CNNモデルで原画像を入力して、力業で判別するのもいいですよ。”

![imageSMR2-44-11](/2025-01-19-QEUR23_SMNRT44/imageSMR2-44-11.jpg)

D先生 ： “原画像かつ、CNNモデルでの外観検査って、（トライアルを）やってみたくはありませんか？”

QEU:FOUNDER ： “ゲッ！**現画像でのモデル構築は大変**だよ・・・。興味のある人は自己責任でやってみてください。”
